{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习 [简单粗暴 TensorFlow 2](https://tf.wiki/zh_hans/) (github页 https://github.com/snowkylin/tensorflow-handbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = 80\n",
    "pd.options.display.precision = 4\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.float_format = '{:.4f}'.format  # 防止科学计数法，小数显示4位\n",
    "\n",
    "# gpu_devices = tf.config.list_physical_devices('GPU') \n",
    "# if gpu_devices:\n",
    "#     for gpu_device in gpu_devices:\n",
    "#         tf.config.experimental.set_memory_growth(gpu_device, True)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个随机数（标量）\n",
    "random_float = tf.random.uniform(shape=())\n",
    "\n",
    "# 定义一个有2个元素的零向量\n",
    "zero_vector = tf.zeros(shape=(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0038645267>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义两个2×2的常量矩阵\n",
    "A = tf.constant([[1., 2.], [3., 4.]])\n",
    "B = tf.constant([[5., 6.], [7., 8.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "<dtype: 'float32'>\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)\n",
    "print(A.dtype)\n",
    "print(A.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = tf.add(A, B)\n",
    "D = tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 6.,  8.],\n",
       "       [10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[19., 22.],\n",
       "       [43., 50.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:     # 在 tf.GradientTape() 的上下文内，所有计算步骤都会被记录以用于求导\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x)        # 计算y关于x的导数\n",
    "print(y, y_grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w, b)关于w, b的偏导数\n",
    "print(L, w_grad, b_grad, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.36363637, 0.54545456, 0.8181818 , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Numpy线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763702027872221 0.057564988311377796\n"
     ]
    }
   ],
   "source": [
    "a, b = 0, 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 5e-4\n",
    "for e in range(num_epoch):\n",
    "    # 手动计算损失函数关于自变量（模型参数）的梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = 2 * (y_pred - y).dot(X), 2 * (y_pred - y).sum()\n",
    "\n",
    "    # 更新参数\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试Tensorflow线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
    "for e in range(num_epoch):\n",
    "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\n",
    "    # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    # TensorFlow自动根据梯度更新参数\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.97637>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.057565063>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "d = 1 \n",
    "alist = [c, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 2\n",
    "alist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model与Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, tf.Tensor(250.0, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.9      ],\n",
      "       [1.1999999],\n",
      "       [1.5      ]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.29999998], dtype=float32)>]\n",
      "1000, tf.Tensor(1.8959781e-08, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[1.4300377e-04],\n",
      "       [1.1111389e+00],\n",
      "       [2.2221353e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1109966], dtype=float32)>]\n",
      "2000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "3000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "4000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "5000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "6000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "7000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "8000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "9000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.zeros_initializer(),\n",
    "            bias_initializer=tf.zeros_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 以下代码结构与前节类似\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for i in range(10000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)      # 调用模型 y_pred = model(X) 而不是显式写出 y_pred = a * X + b\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, model.variables)    # 使用 model.variables 这一属性直接获得模型中的所有变量\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    if i % 1000 == 0:\n",
    "        print(\"\"\"%s, %s\"\"\" % (i, loss))\n",
    "        print(model.variables)\n",
    "print(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础示例：多层感知机(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 5000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.371371\n",
      "batch 1: loss 2.239149\n",
      "batch 2: loss 2.126992\n",
      "batch 3: loss 2.013624\n",
      "batch 4: loss 1.921539\n",
      "batch 5: loss 1.823450\n",
      "batch 6: loss 1.731531\n",
      "batch 7: loss 1.642982\n",
      "batch 8: loss 1.546291\n",
      "batch 9: loss 1.482331\n",
      "batch 10: loss 1.386470\n",
      "batch 11: loss 1.329714\n",
      "batch 12: loss 1.246488\n",
      "batch 13: loss 1.197361\n",
      "batch 14: loss 1.122247\n",
      "batch 15: loss 1.063440\n",
      "batch 16: loss 0.999299\n",
      "batch 17: loss 0.940590\n",
      "batch 18: loss 0.892957\n",
      "batch 19: loss 0.847923\n",
      "batch 20: loss 0.786904\n",
      "batch 21: loss 0.773646\n",
      "batch 22: loss 0.748165\n",
      "batch 23: loss 0.694927\n",
      "batch 24: loss 0.673480\n",
      "batch 25: loss 0.643737\n",
      "batch 26: loss 0.618235\n",
      "batch 27: loss 0.593064\n",
      "batch 28: loss 0.578121\n",
      "batch 29: loss 0.576151\n",
      "batch 30: loss 0.565602\n",
      "batch 31: loss 0.533897\n",
      "batch 32: loss 0.528404\n",
      "batch 33: loss 0.497020\n",
      "batch 34: loss 0.515296\n",
      "batch 35: loss 0.504322\n",
      "batch 36: loss 0.471791\n",
      "batch 37: loss 0.476796\n",
      "batch 38: loss 0.453334\n",
      "batch 39: loss 0.471725\n",
      "batch 40: loss 0.440841\n",
      "batch 41: loss 0.441390\n",
      "batch 42: loss 0.433406\n",
      "batch 43: loss 0.406765\n",
      "batch 44: loss 0.429305\n",
      "batch 45: loss 0.417058\n",
      "batch 46: loss 0.392048\n",
      "batch 47: loss 0.413232\n",
      "batch 48: loss 0.419876\n",
      "batch 49: loss 0.372667\n",
      "batch 50: loss 0.392747\n",
      "batch 51: loss 0.363084\n",
      "batch 52: loss 0.385624\n",
      "batch 53: loss 0.384982\n",
      "batch 54: loss 0.357665\n",
      "batch 55: loss 0.354329\n",
      "batch 56: loss 0.385607\n",
      "batch 57: loss 0.363460\n",
      "batch 58: loss 0.362886\n",
      "batch 59: loss 0.338398\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 5000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.308586\n",
      "batch 1: loss 2.165733\n",
      "batch 2: loss 2.054898\n",
      "batch 3: loss 1.946550\n",
      "batch 4: loss 1.846834\n",
      "batch 5: loss 1.754010\n",
      "batch 6: loss 1.665236\n",
      "batch 7: loss 1.558420\n",
      "batch 8: loss 1.479116\n",
      "batch 9: loss 1.397051\n",
      "batch 10: loss 1.320507\n",
      "batch 11: loss 1.230818\n",
      "batch 12: loss 1.167060\n",
      "batch 13: loss 1.080948\n",
      "batch 14: loss 1.031197\n",
      "batch 15: loss 0.990011\n",
      "batch 16: loss 0.944606\n",
      "batch 17: loss 0.877118\n",
      "batch 18: loss 0.818900\n",
      "batch 19: loss 0.798874\n",
      "batch 20: loss 0.739151\n",
      "batch 21: loss 0.721938\n",
      "batch 22: loss 0.703488\n",
      "batch 23: loss 0.672889\n",
      "batch 24: loss 0.649885\n",
      "batch 25: loss 0.625202\n",
      "batch 26: loss 0.601108\n",
      "batch 27: loss 0.601640\n",
      "batch 28: loss 0.569000\n",
      "batch 29: loss 0.559953\n",
      "batch 30: loss 0.557702\n",
      "batch 31: loss 0.519960\n",
      "batch 32: loss 0.505457\n",
      "batch 33: loss 0.497832\n",
      "batch 34: loss 0.478130\n",
      "batch 35: loss 0.482033\n",
      "batch 36: loss 0.480359\n",
      "batch 37: loss 0.458188\n",
      "batch 38: loss 0.440691\n",
      "batch 39: loss 0.440099\n",
      "batch 40: loss 0.425644\n",
      "batch 41: loss 0.440526\n",
      "batch 42: loss 0.407720\n",
      "batch 43: loss 0.424148\n",
      "batch 44: loss 0.414222\n",
      "batch 45: loss 0.400554\n",
      "batch 46: loss 0.410310\n",
      "batch 47: loss 0.400375\n",
      "batch 48: loss 0.388048\n",
      "batch 49: loss 0.374759\n",
      "batch 50: loss 0.362770\n",
      "batch 51: loss 0.379464\n",
      "batch 52: loss 0.390220\n",
      "batch 53: loss 0.369204\n",
      "batch 54: loss 0.353606\n",
      "batch 55: loss 0.353208\n",
      "batch 56: loss 0.355975\n",
      "batch 57: loss 0.370658\n",
      "batch 58: loss 0.351837\n",
      "batch 59: loss 0.342912\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=5000.0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=10000.0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.911100\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Keras实现卷积神经网络  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,             # 卷积层神经元（卷积核）数目\n",
    "            kernel_size=[5, 5],     # 感受野大小\n",
    "            padding='same',         # padding策略（vaild 或 same）\n",
    "            activation=tf.nn.relu   # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 5000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.298971\n",
      "batch 1: loss 2.056868\n",
      "batch 2: loss 1.746237\n",
      "batch 3: loss 1.402286\n",
      "batch 4: loss 1.067504\n",
      "batch 5: loss 0.837676\n",
      "batch 6: loss 0.769545\n",
      "batch 7: loss 0.755532\n",
      "batch 8: loss 0.576172\n",
      "batch 9: loss 0.634142\n",
      "batch 10: loss 0.500004\n",
      "batch 11: loss 0.560528\n",
      "batch 12: loss 0.439404\n",
      "batch 13: loss 0.402138\n",
      "batch 14: loss 0.437225\n",
      "batch 15: loss 0.390562\n",
      "batch 16: loss 0.360635\n",
      "batch 17: loss 0.352598\n",
      "batch 18: loss 0.361211\n",
      "batch 19: loss 0.289392\n",
      "batch 20: loss 0.281713\n",
      "batch 21: loss 0.272801\n",
      "batch 22: loss 0.274740\n",
      "batch 23: loss 0.268170\n",
      "batch 24: loss 0.252186\n",
      "batch 25: loss 0.246066\n",
      "batch 26: loss 0.220906\n",
      "batch 27: loss 0.226074\n",
      "batch 28: loss 0.232279\n",
      "batch 29: loss 0.203930\n",
      "batch 30: loss 0.193038\n",
      "batch 31: loss 0.181264\n",
      "batch 32: loss 0.194855\n",
      "batch 33: loss 0.194136\n",
      "batch 34: loss 0.168026\n",
      "batch 35: loss 0.158129\n",
      "batch 36: loss 0.164509\n",
      "batch 37: loss 0.165041\n",
      "batch 38: loss 0.148151\n",
      "batch 39: loss 0.153146\n",
      "batch 40: loss 0.164807\n",
      "batch 41: loss 0.157485\n",
      "batch 42: loss 0.136613\n",
      "batch 43: loss 0.136190\n",
      "batch 44: loss 0.137726\n",
      "batch 45: loss 0.138117\n",
      "batch 46: loss 0.137677\n",
      "batch 47: loss 0.120426\n",
      "batch 48: loss 0.121596\n",
      "batch 49: loss 0.120247\n",
      "batch 50: loss 0.111546\n",
      "batch 51: loss 0.123255\n",
      "batch 52: loss 0.105963\n",
      "batch 53: loss 0.113098\n",
      "batch 54: loss 0.108601\n",
      "batch 55: loss 0.097730\n",
      "batch 56: loss 0.097488\n",
      "batch 57: loss 0.097028\n",
      "batch 58: loss 0.099927\n",
      "batch 59: loss 0.088861\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=5000.0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=10000.0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.974400\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
