{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习 [简单粗暴 TensorFlow 2](https://tf.wiki/zh_hans/) (github页 https://github.com/snowkylin/tensorflow-handbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = 80\n",
    "pd.options.display.precision = 4\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.float_format = '{:.4f}'.format  # 防止科学计数法，小数显示4位\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU') \n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu_device, True)\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个随机数（标量）\n",
    "random_float = tf.random.uniform(shape=())\n",
    "\n",
    "# 定义一个有2个元素的零向量\n",
    "zero_vector = tf.zeros(shape=(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.2507789>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义两个2×2的常量矩阵\n",
    "A = tf.constant([[1., 2.], [3., 4.]])\n",
    "B = tf.constant([[5., 6.], [7., 8.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "<dtype: 'float32'>\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)\n",
    "print(A.dtype)\n",
    "print(A.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = tf.add(A, B)\n",
    "D = tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 6.,  8.],\n",
       "       [10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[19., 22.],\n",
       "       [43., 50.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:     # 在 tf.GradientTape() 的上下文内，所有计算步骤都会被记录以用于求导\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x)        # 计算y关于x的导数\n",
    "print(y, y_grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w, b)关于w, b的偏导数\n",
    "print(L, w_grad, b_grad, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.36363637, 0.54545456, 0.8181818 , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Numpy线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763702027872221 0.057564988311377796\n"
     ]
    }
   ],
   "source": [
    "a, b = 0, 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 5e-4\n",
    "for e in range(num_epoch):\n",
    "    # 手动计算损失函数关于自变量（模型参数）的梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = 2 * (y_pred - y).dot(X), 2 * (y_pred - y).sum()\n",
    "\n",
    "    # 更新参数\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试Tensorflow线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
    "for e in range(num_epoch):\n",
    "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\n",
    "    # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    # TensorFlow自动根据梯度更新参数\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.97637>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.057565063>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "d = 1 \n",
    "alist = [c, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 2\n",
    "alist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model与Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, tf.Tensor(250.0, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.9      ],\n",
      "       [1.1999999],\n",
      "       [1.5      ]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.29999998], dtype=float32)>]\n",
      "1000, tf.Tensor(1.8959781e-08, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[1.4300377e-04],\n",
      "       [1.1111389e+00],\n",
      "       [2.2221353e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1109966], dtype=float32)>]\n",
      "2000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "3000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "4000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "5000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "6000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "7000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "8000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "9000, tf.Tensor(3.092282e-11, shape=(), dtype=float32)\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n",
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[4.8735565e-06],\n",
      "       [1.1111153e+00],\n",
      "       [2.2222164e+00]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.1111081], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.zeros_initializer(),\n",
    "            bias_initializer=tf.zeros_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 以下代码结构与前节类似\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for i in range(10000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)      # 调用模型 y_pred = model(X) 而不是显式写出 y_pred = a * X + b\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, model.variables)    # 使用 model.variables 这一属性直接获得模型中的所有变量\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    if i % 1000 == 0:\n",
    "        print(\"\"\"%s, %s\"\"\" % (i, loss))\n",
    "        print(model.variables)\n",
    "print(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础示例：多层感知机(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 5000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.310311\n",
      "batch 1: loss 2.182139\n",
      "batch 2: loss 2.060784\n",
      "batch 3: loss 1.959442\n",
      "batch 4: loss 1.858464\n",
      "batch 5: loss 1.757245\n",
      "batch 6: loss 1.657469\n",
      "batch 7: loss 1.582313\n",
      "batch 8: loss 1.488093\n",
      "batch 9: loss 1.385024\n",
      "batch 10: loss 1.298926\n",
      "batch 11: loss 1.236500\n",
      "batch 12: loss 1.170656\n",
      "batch 13: loss 1.107809\n",
      "batch 14: loss 1.032902\n",
      "batch 15: loss 1.008354\n",
      "batch 16: loss 0.942107\n",
      "batch 17: loss 0.878711\n",
      "batch 18: loss 0.847123\n",
      "batch 19: loss 0.815416\n",
      "batch 20: loss 0.760611\n",
      "batch 21: loss 0.748718\n",
      "batch 22: loss 0.710711\n",
      "batch 23: loss 0.700457\n",
      "batch 24: loss 0.664940\n",
      "batch 25: loss 0.646776\n",
      "batch 26: loss 0.618817\n",
      "batch 27: loss 0.608039\n",
      "batch 28: loss 0.587438\n",
      "batch 29: loss 0.586172\n",
      "batch 30: loss 0.559860\n",
      "batch 31: loss 0.537126\n",
      "batch 32: loss 0.537327\n",
      "batch 33: loss 0.528377\n",
      "batch 34: loss 0.510048\n",
      "batch 35: loss 0.480038\n",
      "batch 36: loss 0.497400\n",
      "batch 37: loss 0.465899\n",
      "batch 38: loss 0.480509\n",
      "batch 39: loss 0.445201\n",
      "batch 40: loss 0.453532\n",
      "batch 41: loss 0.452634\n",
      "batch 42: loss 0.443200\n",
      "batch 43: loss 0.417594\n",
      "batch 44: loss 0.425118\n",
      "batch 45: loss 0.412327\n",
      "batch 46: loss 0.427679\n",
      "batch 47: loss 0.417383\n",
      "batch 48: loss 0.404320\n",
      "batch 49: loss 0.403572\n",
      "batch 50: loss 0.387538\n",
      "batch 51: loss 0.401356\n",
      "batch 52: loss 0.374742\n",
      "batch 53: loss 0.390531\n",
      "batch 54: loss 0.371785\n",
      "batch 55: loss 0.391211\n",
      "batch 56: loss 0.378679\n",
      "batch 57: loss 0.375107\n",
      "batch 58: loss 0.365089\n",
      "batch 59: loss 0.355147\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 5000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.309474\n",
      "batch 1: loss 2.184384\n",
      "batch 2: loss 2.067222\n",
      "batch 3: loss 1.961653\n",
      "batch 4: loss 1.848184\n",
      "batch 5: loss 1.751510\n",
      "batch 6: loss 1.665823\n",
      "batch 7: loss 1.567607\n",
      "batch 8: loss 1.469888\n",
      "batch 9: loss 1.385012\n",
      "batch 10: loss 1.311684\n",
      "batch 11: loss 1.236081\n",
      "batch 12: loss 1.167836\n",
      "batch 13: loss 1.098149\n",
      "batch 14: loss 1.043209\n",
      "batch 15: loss 0.991133\n",
      "batch 16: loss 0.934995\n",
      "batch 17: loss 0.893430\n",
      "batch 18: loss 0.846296\n",
      "batch 19: loss 0.821185\n",
      "batch 20: loss 0.773384\n",
      "batch 21: loss 0.760975\n",
      "batch 22: loss 0.720968\n",
      "batch 23: loss 0.695151\n",
      "batch 24: loss 0.665964\n",
      "batch 25: loss 0.646742\n",
      "batch 26: loss 0.619782\n",
      "batch 27: loss 0.610285\n",
      "batch 28: loss 0.589692\n",
      "batch 29: loss 0.573173\n",
      "batch 30: loss 0.552676\n",
      "batch 31: loss 0.553706\n",
      "batch 32: loss 0.531903\n",
      "batch 33: loss 0.520633\n",
      "batch 34: loss 0.495754\n",
      "batch 35: loss 0.488248\n",
      "batch 36: loss 0.497202\n",
      "batch 37: loss 0.478454\n",
      "batch 38: loss 0.481091\n",
      "batch 39: loss 0.469034\n",
      "batch 40: loss 0.480349\n",
      "batch 41: loss 0.444983\n",
      "batch 42: loss 0.454937\n",
      "batch 43: loss 0.434779\n",
      "batch 44: loss 0.426465\n",
      "batch 45: loss 0.416020\n",
      "batch 46: loss 0.433054\n",
      "batch 47: loss 0.402640\n",
      "batch 48: loss 0.412590\n",
      "batch 49: loss 0.404287\n",
      "batch 50: loss 0.400711\n",
      "batch 51: loss 0.410749\n",
      "batch 52: loss 0.378450\n",
      "batch 53: loss 0.381000\n",
      "batch 54: loss 0.400428\n",
      "batch 55: loss 0.376939\n",
      "batch 56: loss 0.374541\n",
      "batch 57: loss 0.367832\n",
      "batch 58: loss 0.370319\n",
      "batch 59: loss 0.362422\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=5000.0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=10000.0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.905600\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9], dtype=int32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.test_label[start_index: end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Keras实现卷积神经网络  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,             # 卷积层神经元（卷积核）数目\n",
    "            kernel_size=[5, 5],     # 感受野大小\n",
    "            padding='same',         # padding策略（vaild 或 same）\n",
    "            activation=tf.nn.relu   # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 5000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.302025\n",
      "batch 1: loss 2.058893\n",
      "batch 2: loss 1.747453\n",
      "batch 3: loss 1.366509\n",
      "batch 4: loss 1.053738\n",
      "batch 5: loss 0.909884\n",
      "batch 6: loss 0.789708\n",
      "batch 7: loss 0.798355\n",
      "batch 8: loss 0.688380\n",
      "batch 9: loss 0.577808\n",
      "batch 10: loss 0.609948\n",
      "batch 11: loss 0.455673\n",
      "batch 12: loss 0.464754\n",
      "batch 13: loss 0.437746\n",
      "batch 14: loss 0.402453\n",
      "batch 15: loss 0.407344\n",
      "batch 16: loss 0.355393\n",
      "batch 17: loss 0.328806\n",
      "batch 18: loss 0.370193\n",
      "batch 19: loss 0.334883\n",
      "batch 20: loss 0.298504\n",
      "batch 21: loss 0.285496\n",
      "batch 22: loss 0.288759\n",
      "batch 23: loss 0.289981\n",
      "batch 24: loss 0.263217\n",
      "batch 25: loss 0.269641\n",
      "batch 26: loss 0.248456\n",
      "batch 27: loss 0.251506\n",
      "batch 28: loss 0.238241\n",
      "batch 29: loss 0.203239\n",
      "batch 30: loss 0.209931\n",
      "batch 31: loss 0.203775\n",
      "batch 32: loss 0.206832\n",
      "batch 33: loss 0.188401\n",
      "batch 34: loss 0.190474\n",
      "batch 35: loss 0.184831\n",
      "batch 36: loss 0.171019\n",
      "batch 37: loss 0.177792\n",
      "batch 38: loss 0.169812\n",
      "batch 39: loss 0.160875\n",
      "batch 40: loss 0.176795\n",
      "batch 41: loss 0.160044\n",
      "batch 42: loss 0.152248\n",
      "batch 43: loss 0.148017\n",
      "batch 44: loss 0.142270\n",
      "batch 45: loss 0.148917\n",
      "batch 46: loss 0.128637\n",
      "batch 47: loss 0.139748\n",
      "batch 48: loss 0.121090\n",
      "batch 49: loss 0.121600\n",
      "batch 50: loss 0.116477\n",
      "batch 51: loss 0.115995\n",
      "batch 52: loss 0.120366\n",
      "batch 53: loss 0.124024\n",
      "batch 54: loss 0.115638\n",
      "batch 55: loss 0.112890\n",
      "batch 56: loss 0.096828\n",
      "batch 57: loss 0.097883\n",
      "batch 58: loss 0.103483\n",
      "batch 59: loss 0.099509\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=5000.0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=10000.0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.973000\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Keras中预定义的经典卷积神经网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.MobileNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-25f6cff36a6c>:1: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.set_learning_phase(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.671574\n",
      "loss 1.491288\n",
      "loss 1.802438\n",
      "loss 1.968150\n",
      "loss 1.493334\n",
      "loss 1.730590\n",
      "loss 2.254787\n",
      "loss 2.130812\n",
      "loss 1.731451\n",
      "loss 1.261857\n",
      "loss 2.667665\n",
      "loss 2.557539\n",
      "loss 1.690144\n",
      "loss 1.653540\n",
      "loss 1.529181\n",
      "loss 2.247490\n",
      "loss 1.650390\n",
      "loss 1.810979\n",
      "loss 1.742309\n",
      "loss 1.821816\n",
      "loss 1.339355\n",
      "loss 1.857087\n",
      "loss 2.090892\n",
      "loss 1.110196\n",
      "loss 2.735552\n",
      "loss 2.317418\n",
      "loss 1.800508\n",
      "loss 2.048780\n",
      "loss 1.487516\n",
      "loss 1.959415\n",
      "loss 1.665091\n",
      "loss 0.866619\n",
      "loss 1.321856\n",
      "loss 1.799621\n",
      "loss 1.199245\n",
      "loss 0.935215\n",
      "loss 2.830135\n",
      "loss 2.218498\n",
      "loss 1.512071\n",
      "loss 1.620365\n",
      "loss 1.374629\n",
      "loss 1.502180\n",
      "loss 1.181385\n",
      "loss 1.073298\n",
      "loss 1.214451\n",
      "loss 2.018344\n",
      "loss 1.715464\n",
      "loss 2.043723\n",
      "loss 1.562713\n",
      "loss 1.236320\n",
      "loss 1.474574\n",
      "loss 1.614066\n",
      "loss 1.369063\n",
      "loss 1.242561\n",
      "loss 1.390357\n",
      "loss 1.621014\n",
      "loss 1.571876\n",
      "loss 1.546423\n",
      "loss 1.347420\n",
      "loss 1.480775\n",
      "loss 1.940262\n",
      "loss 1.135460\n",
      "loss 1.444464\n",
      "loss 1.099991\n",
      "loss 1.502466\n",
      "loss 1.773658\n",
      "loss 1.558560\n",
      "loss 1.424176\n",
      "loss 1.781923\n",
      "loss 1.133695\n",
      "loss 1.434603\n",
      "loss 2.063267\n",
      "loss 1.565461\n",
      "loss 1.444790\n",
      "loss 1.401687\n",
      "loss 1.878341\n",
      "loss 1.461519\n",
      "loss 1.387517\n",
      "loss 1.481981\n",
      "loss 1.365164\n",
      "loss 1.407005\n",
      "loss 1.438924\n",
      "loss 1.450110\n",
      "loss 1.387149\n",
      "loss 1.618353\n",
      "loss 1.555160\n",
      "loss 1.341326\n",
      "loss 1.682159\n",
      "loss 1.139187\n",
      "loss 1.542077\n",
      "loss 1.169293\n",
      "loss 1.944670\n",
      "loss 1.346635\n",
      "loss 1.541639\n",
      "loss 1.767563\n",
      "loss 1.977731\n",
      "loss 1.564057\n",
      "loss 1.789534\n",
      "loss 1.464776\n",
      "loss 1.534401\n",
      "loss 1.510455\n",
      "loss 1.637112\n",
      "loss 1.677877\n",
      "loss 1.563775\n",
      "loss 2.167440\n",
      "loss 1.623803\n",
      "loss 1.214994\n",
      "loss 1.557513\n",
      "loss 1.306874\n",
      "loss 1.381674\n",
      "loss 1.046212\n",
      "loss 1.118030\n",
      "loss 1.266061\n",
      "loss 1.451971\n",
      "loss 1.276184\n",
      "loss 1.891176\n",
      "loss 1.197407\n",
      "loss 1.464060\n",
      "loss 1.263908\n",
      "loss 1.519590\n",
      "loss 2.087569\n",
      "loss 1.885061\n",
      "loss 1.993414\n",
      "loss 1.335548\n",
      "loss 1.800117\n",
      "loss 1.263003\n",
      "loss 1.423932\n",
      "loss 1.628783\n",
      "loss 1.366076\n",
      "loss 1.743314\n",
      "loss 1.426854\n",
      "loss 1.742728\n",
      "loss 1.272220\n",
      "loss 1.341063\n",
      "loss 1.470754\n",
      "loss 1.044060\n",
      "loss 1.221773\n",
      "loss 1.094467\n",
      "loss 1.157831\n",
      "loss 1.314471\n",
      "loss 1.519101\n",
      "loss 1.780829\n",
      "loss 1.313364\n",
      "loss 1.174222\n",
      "loss 1.629529\n",
      "loss 1.321942\n",
      "loss 1.045597\n",
      "loss 1.294457\n",
      "loss 1.754542\n",
      "loss 1.510631\n",
      "loss 1.253086\n",
      "loss 1.302664\n",
      "loss 1.288370\n",
      "loss 1.338366\n",
      "loss 1.287217\n",
      "loss 1.593171\n",
      "loss 1.259878\n",
      "loss 1.173623\n",
      "loss 1.482315\n",
      "loss 1.109727\n",
      "loss 1.468958\n",
      "loss 1.094364\n",
      "loss 1.171210\n",
      "loss 1.073421\n",
      "loss 1.297797\n",
      "loss 1.124254\n",
      "loss 1.162412\n",
      "loss 1.169630\n",
      "loss 2.231887\n",
      "loss 1.057174\n",
      "loss 1.362964\n",
      "loss 0.974265\n",
      "loss 1.467568\n",
      "loss 1.359244\n",
      "loss 0.990651\n",
      "loss 1.198681\n",
      "loss 1.489343\n",
      "loss 1.550481\n",
      "loss 1.785365\n",
      "loss 1.099986\n",
      "loss 1.363210\n",
      "loss 1.750084\n",
      "loss 1.251340\n",
      "loss 1.392570\n",
      "loss 1.876773\n",
      "loss 1.212912\n",
      "loss 1.123028\n",
      "loss 1.538333\n",
      "loss 1.274759\n",
      "loss 1.506444\n",
      "loss 1.023814\n",
      "loss 1.236370\n",
      "loss 1.708057\n",
      "loss 1.100519\n",
      "loss 1.175382\n",
      "loss 1.918771\n",
      "loss 1.428843\n",
      "loss 0.985854\n",
      "loss 1.444056\n",
      "loss 1.079849\n",
      "loss 1.958545\n",
      "loss 0.865944\n",
      "loss 1.538090\n",
      "loss 1.045170\n",
      "loss 1.948107\n",
      "loss 1.246955\n",
      "loss 0.946558\n",
      "loss 1.518696\n",
      "loss 1.810098\n",
      "loss 1.222052\n",
      "loss 1.545464\n",
      "loss 1.288171\n",
      "loss 1.460147\n",
      "loss 1.243284\n",
      "loss 1.529755\n",
      "loss 2.004474\n",
      "loss 1.406240\n",
      "loss 1.642479\n",
      "loss 1.532447\n",
      "loss 1.243434\n",
      "loss 1.616838\n",
      "loss 1.561466\n",
      "loss 1.226909\n",
      "loss 1.347445\n",
      "loss 1.313671\n",
      "loss 0.975896\n",
      "loss 1.242498\n",
      "loss 1.415492\n",
      "loss 1.499105\n",
      "loss 1.313769\n",
      "loss 1.554858\n",
      "loss 1.741347\n",
      "loss 1.106429\n",
      "loss 1.347277\n",
      "loss 1.245155\n",
      "loss 0.926466\n",
      "loss 1.424154\n",
      "loss 0.787868\n",
      "loss 1.359629\n",
      "loss 2.036662\n",
      "loss 1.344254\n",
      "loss 0.901989\n",
      "loss 1.056171\n",
      "loss 1.455153\n",
      "loss 1.175426\n",
      "loss 1.193616\n",
      "loss 1.256026\n",
      "loss 0.968226\n",
      "loss 1.136184\n",
      "loss 0.772936\n",
      "loss 1.581337\n",
      "loss 0.954768\n",
      "loss 1.913776\n",
      "loss 0.946875\n",
      "loss 2.441702\n",
      "loss 1.236310\n",
      "loss 1.336303\n",
      "loss 1.385752\n",
      "loss 2.198449\n",
      "loss 1.485011\n",
      "loss 1.046654\n",
      "loss 1.093004\n",
      "loss 1.523928\n",
      "loss 1.466282\n",
      "loss 1.253807\n",
      "loss 1.134710\n",
      "loss 1.515876\n",
      "loss 1.134261\n",
      "loss 1.112541\n",
      "loss 1.244662\n",
      "loss 1.774955\n",
      "loss 0.964613\n",
      "loss 1.545594\n",
      "loss 0.591910\n",
      "loss 2.061279\n",
      "loss 0.718063\n",
      "loss 1.580059\n",
      "loss 1.361247\n",
      "loss 1.220820\n",
      "loss 0.724915\n",
      "loss 1.300863\n",
      "loss 1.802183\n",
      "loss 1.427119\n",
      "loss 1.124461\n",
      "loss 1.511975\n",
      "loss 1.463375\n",
      "loss 1.059614\n",
      "loss 1.882154\n",
      "loss 0.981515\n",
      "loss 0.938893\n",
      "loss 1.337505\n",
      "loss 1.156048\n",
      "loss 0.994815\n",
      "loss 1.330576\n",
      "loss 1.205429\n",
      "loss 1.169849\n",
      "loss 1.339315\n",
      "loss 1.196427\n",
      "loss 1.312592\n",
      "loss 0.920232\n",
      "loss 1.433387\n",
      "loss 1.278615\n",
      "loss 1.347727\n",
      "loss 1.446493\n",
      "loss 0.797036\n",
      "loss 1.278872\n",
      "loss 1.703102\n",
      "loss 1.184053\n",
      "loss 1.745625\n",
      "loss 1.145044\n",
      "loss 1.243577\n",
      "loss 1.945219\n",
      "loss 0.960965\n",
      "loss 1.166330\n",
      "loss 0.832873\n",
      "loss 1.374332\n",
      "loss 1.279139\n",
      "loss 1.257794\n",
      "loss 1.020351\n",
      "loss 1.201527\n",
      "loss 1.651402\n",
      "loss 1.727906\n",
      "loss 1.367247\n",
      "loss 1.616466\n",
      "loss 1.236409\n",
      "loss 1.459619\n",
      "loss 1.215624\n",
      "loss 1.185430\n",
      "loss 1.198755\n",
      "loss 0.747888\n",
      "loss 1.546729\n",
      "loss 1.423948\n",
      "loss 1.086493\n",
      "loss 0.890934\n",
      "loss 1.609418\n",
      "loss 0.866691\n",
      "loss 1.599362\n",
      "loss 0.757204\n",
      "loss 1.687932\n",
      "loss 0.754641\n",
      "loss 0.788880\n",
      "loss 1.439845\n",
      "loss 1.516229\n",
      "loss 1.383464\n",
      "loss 1.258694\n",
      "loss 1.078567\n",
      "loss 1.374605\n",
      "loss 1.357936\n",
      "loss 1.258600\n",
      "loss 1.118604\n",
      "loss 1.532828\n",
      "loss 1.513386\n",
      "loss 0.835252\n",
      "loss 1.378335\n",
      "loss 1.677146\n",
      "loss 1.389734\n",
      "loss 1.587309\n",
      "loss 1.769782\n",
      "loss 1.659070\n",
      "loss 1.638100\n",
      "loss 1.412259\n",
      "loss 1.442411\n",
      "loss 1.270046\n",
      "loss 1.391539\n",
      "loss 1.287616\n",
      "loss 1.080789\n",
      "loss 1.958116\n",
      "tf.Tensor(\n",
      "[[0.6111926  0.12963635 0.02975109 0.15501748 0.07440251]\n",
      " [0.21440329 0.38689464 0.07229342 0.1082793  0.2181293 ]\n",
      " [0.34458554 0.24989358 0.07100714 0.15196307 0.18255067]\n",
      " [0.7853553  0.09486562 0.01774763 0.04973754 0.05229391]\n",
      " [0.24805462 0.18997952 0.10153658 0.2540884  0.2063409 ]\n",
      " [0.0588172  0.07225253 0.20261906 0.29954758 0.36676365]\n",
      " [0.00141341 0.02397344 0.13628082 0.01029558 0.8280368 ]\n",
      " [0.05565646 0.0250167  0.23291662 0.24400404 0.44240615]\n",
      " [0.25721657 0.34027776 0.07156137 0.14903146 0.18191276]\n",
      " [0.10103323 0.12126771 0.09194443 0.55555695 0.13019769]], shape=(10, 5), dtype=float32)\n",
      "loss 1.574564\n",
      "loss 0.935284\n",
      "loss 1.092135\n",
      "loss 1.347064\n",
      "loss 1.093594\n",
      "loss 1.613142\n",
      "loss 1.095009\n",
      "loss 1.358425\n",
      "loss 0.719929\n",
      "loss 1.340932\n",
      "loss 0.925084\n",
      "loss 0.969436\n",
      "loss 1.236619\n",
      "loss 1.183115\n",
      "loss 2.082690\n",
      "loss 1.925455\n",
      "loss 1.251292\n",
      "loss 1.394082\n",
      "loss 0.702951\n",
      "loss 1.375050\n",
      "loss 2.414648\n",
      "loss 1.229288\n",
      "loss 1.046291\n",
      "loss 1.277938\n",
      "loss 1.009276\n",
      "loss 1.488422\n",
      "loss 1.551151\n",
      "loss 1.513347\n",
      "loss 0.883671\n",
      "loss 0.998915\n",
      "loss 1.144204\n",
      "loss 1.567763\n",
      "loss 1.527875\n",
      "loss 1.177245\n",
      "loss 1.568092\n",
      "loss 1.271871\n",
      "loss 1.200599\n",
      "loss 1.418810\n",
      "loss 1.501463\n",
      "loss 1.540019\n",
      "loss 1.600859\n",
      "loss 1.316184\n",
      "loss 1.088648\n",
      "loss 1.446734\n",
      "loss 1.345844\n",
      "loss 1.055281\n",
      "loss 1.789884\n",
      "loss 1.530490\n",
      "loss 0.988802\n",
      "loss 1.947688\n",
      "loss 1.215863\n",
      "loss 1.096087\n",
      "loss 1.482904\n",
      "loss 0.973208\n",
      "loss 0.972003\n",
      "loss 1.469676\n",
      "loss 1.699743\n",
      "loss 1.280865\n",
      "loss 1.366827\n",
      "loss 1.109349\n",
      "loss 1.071387\n",
      "loss 1.042234\n",
      "loss 1.538980\n",
      "loss 1.174172\n",
      "loss 1.476458\n",
      "loss 1.083172\n",
      "loss 1.113787\n",
      "loss 1.137021\n",
      "loss 1.154869\n",
      "loss 1.600692\n",
      "loss 1.070618\n",
      "loss 1.338689\n",
      "loss 1.130917\n",
      "loss 0.721239\n",
      "loss 1.619955\n",
      "loss 2.521134\n",
      "loss 1.971856\n",
      "loss 1.355730\n",
      "loss 1.092925\n",
      "loss 0.773995\n",
      "loss 1.528262\n",
      "loss 0.676997\n",
      "loss 1.063222\n",
      "loss 1.313166\n",
      "loss 0.974362\n",
      "loss 1.496736\n",
      "loss 1.849856\n",
      "loss 1.591428\n",
      "loss 1.433375\n",
      "loss 1.190132\n",
      "loss 1.508917\n",
      "loss 0.887891\n",
      "loss 0.970731\n",
      "loss 1.055832\n",
      "loss 1.036704\n",
      "loss 1.866844\n",
      "loss 1.265288\n",
      "loss 1.250079\n",
      "loss 1.484975\n",
      "loss 0.843999\n",
      "loss 0.939290\n",
      "loss 1.512507\n",
      "loss 1.133446\n",
      "loss 1.248766\n",
      "loss 0.907465\n",
      "loss 0.894793\n",
      "loss 0.911615\n",
      "loss 1.279343\n",
      "loss 1.246356\n",
      "loss 1.742280\n",
      "loss 1.039242\n",
      "loss 1.158158\n",
      "loss 0.897121\n",
      "loss 1.083705\n",
      "loss 0.763430\n",
      "loss 0.699352\n",
      "loss 1.609213\n",
      "loss 0.956849\n",
      "loss 1.096559\n",
      "loss 1.303716\n",
      "loss 1.279935\n",
      "loss 1.181834\n",
      "loss 0.617553\n",
      "loss 1.085688\n",
      "loss 1.013622\n",
      "loss 0.837590\n",
      "loss 1.007434\n",
      "loss 0.814131\n",
      "loss 1.768906\n",
      "loss 1.161503\n",
      "loss 1.256626\n",
      "loss 1.037316\n",
      "loss 1.093031\n",
      "loss 1.166000\n",
      "loss 1.252924\n",
      "loss 1.210148\n",
      "loss 0.920539\n",
      "loss 1.858974\n",
      "loss 0.810466\n",
      "loss 1.551670\n",
      "loss 1.246632\n",
      "loss 1.173171\n",
      "loss 1.026992\n",
      "loss 1.094944\n",
      "loss 0.954875\n",
      "loss 1.428770\n",
      "loss 1.349154\n",
      "loss 1.163568\n",
      "loss 1.258232\n",
      "loss 1.845577\n",
      "loss 0.850651\n",
      "loss 1.153478\n",
      "loss 1.244955\n",
      "loss 1.299142\n",
      "loss 1.157659\n",
      "loss 1.338466\n",
      "loss 1.141054\n",
      "loss 1.133946\n",
      "loss 1.102023\n",
      "loss 0.738316\n",
      "loss 0.792884\n",
      "loss 1.262048\n",
      "loss 1.028791\n",
      "loss 1.262565\n",
      "loss 1.308092\n",
      "loss 0.939232\n",
      "loss 1.400661\n",
      "loss 1.620152\n",
      "loss 1.949012\n",
      "loss 1.202210\n",
      "loss 1.126883\n",
      "loss 1.086660\n",
      "loss 0.992304\n",
      "loss 1.314066\n",
      "loss 1.116988\n",
      "loss 1.479338\n",
      "loss 0.941671\n",
      "loss 2.004174\n",
      "loss 1.091691\n",
      "loss 1.022018\n",
      "loss 1.756146\n",
      "loss 1.235250\n",
      "loss 1.514564\n",
      "loss 1.305482\n",
      "loss 0.721778\n",
      "loss 1.222167\n",
      "loss 1.265310\n",
      "loss 1.284432\n",
      "loss 1.453441\n",
      "loss 1.255671\n",
      "loss 1.089688\n",
      "loss 1.251009\n",
      "loss 1.094211\n",
      "loss 1.440502\n",
      "loss 0.872251\n",
      "loss 1.427575\n",
      "loss 1.233474\n",
      "loss 1.160237\n",
      "loss 1.116612\n",
      "loss 0.927843\n",
      "loss 0.836228\n",
      "loss 0.994961\n",
      "loss 0.958121\n",
      "loss 0.923025\n",
      "loss 1.158931\n",
      "loss 1.555796\n",
      "loss 0.909732\n",
      "loss 1.305606\n",
      "loss 1.303026\n",
      "loss 1.166054\n",
      "loss 0.898840\n",
      "loss 1.113627\n",
      "loss 1.449880\n",
      "loss 1.004868\n",
      "loss 1.745785\n",
      "loss 1.234435\n",
      "loss 1.243658\n",
      "loss 0.930080\n",
      "loss 1.427034\n",
      "loss 1.526546\n",
      "loss 1.092419\n",
      "loss 1.068768\n",
      "loss 1.534815\n",
      "loss 1.332802\n",
      "loss 0.820758\n",
      "loss 1.484490\n",
      "loss 1.518844\n",
      "loss 1.095079\n",
      "loss 1.244099\n",
      "loss 0.728269\n",
      "loss 1.451806\n",
      "loss 0.863808\n",
      "loss 0.995572\n",
      "loss 0.893174\n",
      "loss 0.724691\n",
      "loss 1.250862\n",
      "loss 0.995571\n",
      "loss 0.953019\n",
      "loss 1.347185\n",
      "loss 1.039020\n",
      "loss 1.321109\n",
      "loss 1.244602\n",
      "loss 1.154380\n",
      "loss 2.092644\n",
      "loss 1.038951\n",
      "loss 0.760142\n",
      "loss 1.423173\n",
      "loss 1.147966\n",
      "loss 1.585220\n",
      "loss 0.866966\n",
      "loss 1.072249\n",
      "loss 1.128677\n",
      "loss 1.161987\n",
      "loss 0.706470\n",
      "loss 1.427056\n",
      "loss 0.945196\n",
      "loss 1.064574\n",
      "loss 0.817220\n",
      "loss 1.556638\n",
      "loss 1.203292\n",
      "loss 1.031566\n",
      "loss 1.359769\n",
      "loss 0.984610\n",
      "loss 0.985105\n",
      "loss 1.087266\n",
      "loss 0.701330\n",
      "loss 1.272505\n",
      "loss 0.865164\n",
      "loss 1.623881\n",
      "loss 1.682661\n",
      "loss 1.390244\n",
      "loss 0.904342\n",
      "loss 1.101390\n",
      "loss 1.239429\n",
      "loss 0.793224\n",
      "loss 1.365953\n",
      "loss 0.851313\n",
      "loss 0.728177\n",
      "loss 1.668450\n",
      "loss 0.521645\n",
      "loss 0.697534\n",
      "loss 1.912477\n",
      "loss 1.281976\n",
      "loss 1.181585\n",
      "loss 0.547713\n",
      "loss 0.950182\n",
      "loss 1.152564\n",
      "loss 1.072777\n",
      "loss 1.187737\n",
      "loss 0.703149\n",
      "loss 0.963417\n",
      "loss 0.836840\n",
      "loss 0.570450\n",
      "loss 0.814487\n",
      "loss 0.746010\n",
      "loss 1.062713\n",
      "loss 1.826128\n",
      "loss 1.204479\n",
      "loss 1.392104\n",
      "loss 0.942633\n",
      "loss 0.930145\n",
      "loss 0.958437\n",
      "loss 1.388436\n",
      "loss 1.140248\n",
      "loss 0.987477\n",
      "loss 1.842794\n",
      "loss 0.956288\n",
      "loss 0.422877\n",
      "loss 0.680456\n",
      "loss 0.477999\n",
      "loss 1.285077\n",
      "loss 0.522911\n",
      "loss 0.757518\n",
      "loss 1.172594\n",
      "loss 1.267807\n",
      "loss 1.143448\n",
      "loss 0.842744\n",
      "loss 1.922649\n",
      "loss 2.287051\n",
      "loss 1.256340\n",
      "loss 0.532486\n",
      "loss 1.484176\n",
      "loss 1.139675\n",
      "loss 1.091444\n",
      "loss 0.948739\n",
      "loss 1.079548\n",
      "loss 1.159126\n",
      "loss 1.519175\n",
      "loss 1.141961\n",
      "loss 1.009523\n",
      "loss 1.206400\n",
      "loss 1.418643\n",
      "loss 1.419013\n",
      "loss 0.911675\n",
      "loss 1.680044\n",
      "loss 0.922488\n",
      "loss 0.780868\n",
      "loss 1.120230\n",
      "loss 1.117497\n",
      "loss 1.320809\n",
      "loss 0.789567\n",
      "loss 1.139369\n",
      "loss 1.137547\n",
      "loss 0.954664\n",
      "loss 1.276026\n",
      "loss 0.977955\n",
      "loss 1.160690\n",
      "loss 1.104258\n",
      "loss 0.942466\n",
      "loss 0.917247\n",
      "loss 1.400218\n",
      "loss 1.043167\n",
      "loss 1.009706\n",
      "loss 1.308883\n",
      "loss 0.884365\n",
      "loss 0.760919\n",
      "loss 0.986874\n",
      "loss 0.767838\n",
      "loss 1.271180\n",
      "loss 1.259199\n",
      "loss 1.125726\n",
      "loss 1.306797\n",
      "loss 0.908706\n",
      "loss 1.421664\n",
      "loss 0.918928\n",
      "loss 1.191690\n",
      "loss 0.533465\n",
      "tf.Tensor(\n",
      "[[1.1873138e-01 3.0678492e-02 3.6911387e-02 8.0575091e-01 7.9278573e-03]\n",
      " [8.1060175e-03 3.9864626e-02 5.6200552e-01 7.8535818e-02 3.1148797e-01]\n",
      " [4.1162944e-03 1.4881171e-02 6.3276380e-01 1.2632374e-01 2.2191504e-01]\n",
      " [4.3350585e-02 4.7351457e-02 4.2911801e-02 8.5458934e-01 1.1796841e-02]\n",
      " [9.6443295e-01 1.6179662e-02 8.2723545e-03 8.2320481e-04 1.0291871e-02]\n",
      " [1.5677781e-03 2.9868071e-03 7.1273583e-01 5.9476469e-02 2.2323300e-01]\n",
      " [2.3441085e-01 5.2877191e-02 6.8857327e-02 6.2951893e-01 1.4335691e-02]\n",
      " [1.2250528e-01 7.1752346e-01 3.5106599e-02 6.6239806e-03 1.1824080e-01]\n",
      " [3.4893498e-01 4.8343965e-01 4.1682657e-02 1.5800223e-02 1.1014250e-01]\n",
      " [1.3667551e-03 2.7368830e-03 7.5566721e-01 9.0431452e-02 1.4979772e-01]], shape=(10, 5), dtype=float32)\n",
      "loss 1.074385\n",
      "loss 0.760099\n",
      "loss 1.270345\n",
      "loss 2.026664\n",
      "loss 1.189394\n",
      "loss 1.711469\n",
      "loss 1.451453\n",
      "loss 0.860428\n",
      "loss 0.901057\n",
      "loss 1.308117\n",
      "loss 0.861095\n",
      "loss 0.567236\n",
      "loss 0.762352\n",
      "loss 0.966656\n",
      "loss 1.093108\n",
      "loss 0.865748\n",
      "loss 1.238455\n",
      "loss 0.926921\n",
      "loss 0.950849\n",
      "loss 0.865183\n",
      "loss 1.142478\n",
      "loss 1.013335\n",
      "loss 0.978205\n",
      "loss 1.191247\n",
      "loss 1.463908\n",
      "loss 1.258779\n",
      "loss 1.436660\n",
      "loss 0.741980\n",
      "loss 1.056000\n",
      "loss 1.170552\n",
      "loss 1.046355\n",
      "loss 0.565003\n",
      "loss 1.150061\n",
      "loss 0.813395\n",
      "loss 1.382802\n",
      "loss 1.320014\n",
      "loss 0.792205\n",
      "loss 1.431943\n",
      "loss 1.341441\n",
      "loss 1.140255\n",
      "loss 1.631241\n",
      "loss 1.012427\n",
      "loss 1.186284\n",
      "loss 1.123406\n",
      "loss 1.038622\n",
      "loss 0.660691\n",
      "loss 0.799380\n",
      "loss 1.291854\n",
      "loss 0.914388\n",
      "loss 0.942110\n",
      "loss 0.766429\n",
      "loss 1.001750\n",
      "loss 0.973515\n",
      "loss 1.127522\n",
      "loss 1.337349\n",
      "loss 1.243411\n",
      "loss 0.730546\n",
      "loss 1.876117\n",
      "loss 1.841023\n",
      "loss 0.992309\n",
      "loss 0.894883\n",
      "loss 1.114604\n",
      "loss 0.875398\n",
      "loss 1.093737\n",
      "loss 1.487083\n",
      "loss 0.850938\n",
      "loss 0.343749\n",
      "loss 1.025506\n",
      "loss 1.250973\n",
      "loss 1.075497\n",
      "loss 1.144486\n",
      "loss 0.875507\n",
      "loss 0.825431\n",
      "loss 1.309943\n",
      "loss 1.744036\n",
      "loss 0.757059\n",
      "loss 0.950821\n",
      "loss 0.944421\n",
      "loss 1.324014\n",
      "loss 1.011528\n",
      "loss 1.472964\n",
      "loss 0.595787\n",
      "loss 0.438454\n",
      "loss 1.089137\n",
      "loss 0.775908\n",
      "loss 1.032108\n",
      "loss 1.037487\n",
      "loss 1.419917\n",
      "loss 1.355597\n",
      "loss 1.280926\n",
      "loss 1.016574\n",
      "loss 1.300147\n",
      "loss 1.247779\n",
      "loss 0.979888\n",
      "loss 1.454368\n",
      "loss 0.860225\n",
      "loss 1.097638\n",
      "loss 0.681639\n",
      "loss 1.541010\n",
      "loss 1.316135\n",
      "loss 1.421846\n",
      "loss 1.056495\n",
      "loss 1.292549\n",
      "loss 1.183852\n",
      "loss 1.048290\n",
      "loss 1.421862\n",
      "loss 0.950929\n",
      "loss 1.461245\n",
      "loss 0.794832\n",
      "loss 1.690289\n",
      "loss 1.060247\n",
      "loss 1.195463\n",
      "loss 1.359179\n",
      "loss 1.027089\n",
      "loss 0.730619\n",
      "loss 1.313864\n",
      "loss 1.562528\n",
      "loss 1.198620\n",
      "loss 0.964682\n",
      "loss 0.876999\n",
      "loss 1.775034\n",
      "loss 0.780258\n",
      "loss 0.831574\n",
      "loss 1.269914\n",
      "loss 1.150055\n",
      "loss 0.667617\n",
      "loss 0.848051\n",
      "loss 1.507466\n",
      "loss 0.619859\n",
      "loss 1.007939\n",
      "loss 0.629784\n",
      "loss 0.570638\n",
      "loss 0.893483\n",
      "loss 0.870417\n",
      "loss 1.626764\n",
      "loss 1.118948\n",
      "loss 1.318939\n",
      "loss 1.290491\n",
      "loss 1.095825\n",
      "loss 1.115966\n",
      "loss 1.460219\n",
      "loss 1.503314\n",
      "loss 0.759146\n",
      "loss 1.547196\n",
      "loss 1.032809\n",
      "loss 1.023355\n",
      "loss 1.222407\n",
      "loss 0.972873\n",
      "loss 0.570637\n",
      "loss 0.987878\n",
      "loss 0.997414\n",
      "loss 1.097764\n",
      "loss 1.024500\n",
      "loss 0.791466\n",
      "loss 1.041039\n",
      "loss 1.594511\n",
      "loss 0.992315\n",
      "loss 0.933969\n",
      "loss 0.647924\n",
      "loss 0.739413\n",
      "loss 1.654946\n",
      "loss 0.676784\n",
      "loss 0.683783\n",
      "loss 1.491598\n",
      "loss 0.894926\n",
      "loss 0.934092\n",
      "loss 1.050448\n",
      "loss 1.288428\n",
      "loss 0.947886\n",
      "loss 1.823127\n",
      "loss 0.852593\n",
      "loss 1.003196\n",
      "loss 1.439818\n",
      "loss 1.691351\n",
      "loss 0.899030\n",
      "loss 1.371596\n",
      "loss 1.465515\n",
      "loss 1.062710\n",
      "loss 0.899576\n",
      "loss 0.818621\n",
      "loss 1.267457\n",
      "loss 0.543954\n",
      "loss 1.297305\n",
      "loss 1.340908\n",
      "loss 1.194693\n",
      "loss 0.737400\n",
      "loss 1.020892\n",
      "loss 0.783192\n",
      "loss 1.372641\n",
      "loss 0.910774\n",
      "loss 1.004134\n",
      "loss 0.984671\n",
      "loss 0.747715\n",
      "loss 1.585227\n",
      "loss 1.106008\n",
      "loss 1.609293\n",
      "loss 0.879020\n",
      "loss 0.777110\n",
      "loss 1.081696\n",
      "loss 0.877670\n",
      "loss 0.950047\n",
      "loss 0.579959\n",
      "loss 0.932306\n",
      "loss 0.851076\n",
      "loss 1.313847\n",
      "loss 0.957283\n",
      "loss 0.981692\n",
      "loss 0.802619\n",
      "loss 1.466461\n",
      "loss 1.144411\n",
      "loss 0.945982\n",
      "loss 0.933908\n",
      "loss 0.731591\n",
      "loss 0.715964\n",
      "loss 0.670329\n",
      "loss 0.613900\n",
      "loss 1.068389\n",
      "loss 0.907339\n",
      "loss 0.958639\n",
      "loss 0.732088\n",
      "loss 1.423848\n",
      "loss 0.909815\n",
      "loss 0.740410\n",
      "loss 0.861735\n",
      "loss 1.717080\n",
      "loss 0.903313\n",
      "loss 1.069025\n",
      "loss 1.208598\n",
      "loss 1.037276\n",
      "loss 1.449153\n",
      "loss 0.880885\n",
      "loss 0.946094\n",
      "loss 0.845215\n",
      "loss 0.585950\n",
      "loss 1.180501\n",
      "loss 1.262050\n",
      "loss 1.185894\n",
      "loss 1.149227\n",
      "loss 0.934319\n",
      "loss 2.149682\n",
      "loss 0.998330\n",
      "loss 0.749885\n",
      "loss 0.927530\n",
      "loss 0.774356\n",
      "loss 1.081937\n",
      "loss 0.944840\n",
      "loss 0.630980\n",
      "loss 0.969250\n",
      "loss 1.238222\n",
      "loss 1.114946\n",
      "loss 0.886507\n",
      "loss 0.751567\n",
      "loss 0.967607\n",
      "loss 1.079366\n",
      "loss 1.484768\n",
      "loss 0.653708\n",
      "loss 1.363192\n",
      "loss 1.324640\n",
      "loss 0.819732\n",
      "loss 0.847844\n",
      "loss 1.289375\n",
      "loss 0.662457\n",
      "loss 0.635589\n",
      "loss 0.837783\n",
      "loss 0.895055\n",
      "loss 1.203134\n",
      "loss 0.897636\n",
      "loss 0.848922\n",
      "loss 1.486671\n",
      "loss 1.068576\n",
      "loss 1.005028\n",
      "loss 1.296555\n",
      "loss 1.230469\n",
      "loss 0.556818\n",
      "loss 1.069823\n",
      "loss 0.590524\n",
      "loss 0.679642\n",
      "loss 1.356730\n",
      "loss 0.999057\n",
      "loss 0.988900\n",
      "loss 0.591224\n",
      "loss 0.717716\n",
      "loss 1.033487\n",
      "loss 1.135218\n",
      "loss 0.892115\n",
      "loss 0.607506\n",
      "loss 0.796680\n",
      "loss 1.546926\n",
      "loss 1.177013\n",
      "loss 0.811361\n",
      "loss 0.586174\n",
      "loss 1.641559\n",
      "loss 1.180953\n",
      "loss 0.850386\n",
      "loss 1.231959\n",
      "loss 0.832887\n",
      "loss 1.004139\n",
      "loss 0.595248\n",
      "loss 1.474509\n",
      "loss 0.664981\n",
      "loss 1.140315\n",
      "loss 1.127256\n",
      "loss 1.109422\n",
      "loss 1.490145\n",
      "loss 1.189863\n",
      "loss 1.341747\n",
      "loss 0.937537\n",
      "loss 0.991103\n",
      "loss 0.904010\n",
      "loss 0.931913\n",
      "loss 0.776520\n",
      "loss 1.028143\n",
      "loss 1.066710\n",
      "loss 0.737783\n",
      "loss 1.266477\n",
      "loss 1.119915\n",
      "loss 0.858335\n",
      "loss 1.163895\n",
      "loss 0.650427\n",
      "loss 1.034040\n",
      "loss 1.027894\n",
      "loss 0.662910\n",
      "loss 1.195728\n",
      "loss 1.434769\n",
      "loss 1.240736\n",
      "loss 0.834062\n",
      "loss 0.801841\n",
      "loss 1.756355\n",
      "loss 0.744833\n",
      "loss 1.323479\n",
      "loss 0.982563\n",
      "loss 0.726562\n",
      "loss 0.888581\n",
      "loss 1.267005\n",
      "loss 0.777397\n",
      "loss 0.875964\n",
      "loss 0.901567\n",
      "loss 1.112191\n",
      "loss 1.427014\n",
      "loss 1.415654\n",
      "loss 0.848582\n",
      "loss 1.422853\n",
      "loss 1.345713\n",
      "loss 0.916964\n",
      "loss 0.875723\n",
      "loss 1.093873\n",
      "loss 1.233218\n",
      "loss 1.498400\n",
      "loss 0.787480\n",
      "loss 0.942760\n",
      "loss 0.581339\n",
      "loss 1.512429\n",
      "loss 1.541781\n",
      "loss 0.634826\n",
      "loss 0.601120\n",
      "loss 1.260775\n",
      "loss 0.663330\n",
      "loss 1.038789\n",
      "loss 0.875903\n",
      "loss 0.984681\n",
      "loss 1.362409\n",
      "loss 1.001588\n",
      "loss 1.364495\n",
      "loss 0.665161\n",
      "loss 0.910644\n",
      "loss 0.730611\n",
      "loss 0.859223\n",
      "tf.Tensor(\n",
      "[[6.97748840e-01 4.47404943e-02 5.05221449e-02 1.84185505e-01\n",
      "  2.28029508e-02]\n",
      " [5.53625822e-01 1.60246521e-01 1.09683871e-01 9.56054628e-02\n",
      "  8.08383226e-02]\n",
      " [1.45956650e-01 1.16968386e-01 2.49106556e-01 4.40237761e-01\n",
      "  4.77306023e-02]\n",
      " [8.01724076e-01 6.62699342e-02 5.01311868e-02 1.66099817e-02\n",
      "  6.52648583e-02]\n",
      " [3.15268128e-03 6.59337174e-03 7.38356471e-01 2.30072252e-03\n",
      "  2.49596789e-01]\n",
      " [2.74288297e-01 2.56331652e-01 2.00546354e-01 7.14820176e-02\n",
      "  1.97351694e-01]\n",
      " [1.20804096e-02 8.82947266e-01 2.39143781e-02 6.16277941e-02\n",
      "  1.94301978e-02]\n",
      " [8.26555610e-01 6.30828142e-02 4.22608666e-02 2.53311135e-02\n",
      "  4.27695625e-02]\n",
      " [1.14295736e-01 4.51104939e-02 7.90816396e-02 7.47174740e-01\n",
      "  1.43373925e-02]\n",
      " [7.47945858e-04 4.48775012e-03 7.25875914e-01 1.60955917e-03\n",
      "  2.67278910e-01]], shape=(10, 5), dtype=float32)\n",
      "loss 1.165333\n",
      "loss 0.991382\n",
      "loss 0.542302\n",
      "loss 0.652023\n",
      "loss 1.420075\n",
      "loss 0.994132\n",
      "loss 0.894566\n",
      "loss 1.065906\n",
      "loss 0.786146\n",
      "loss 0.513218\n",
      "loss 1.055807\n",
      "loss 0.775040\n",
      "loss 0.681342\n",
      "loss 1.151837\n",
      "loss 0.979554\n",
      "loss 0.883228\n",
      "loss 0.704979\n",
      "loss 0.787087\n",
      "loss 0.781758\n",
      "loss 0.393319\n",
      "loss 0.692388\n",
      "loss 1.375780\n",
      "loss 0.582791\n",
      "loss 1.027520\n",
      "loss 1.098975\n",
      "loss 0.741207\n",
      "loss 0.735610\n",
      "loss 0.626613\n",
      "loss 1.242891\n",
      "loss 1.122730\n",
      "loss 1.006817\n",
      "loss 1.524186\n",
      "loss 0.816050\n",
      "loss 1.599647\n",
      "loss 1.338374\n",
      "loss 0.679711\n",
      "loss 0.640592\n",
      "loss 1.336264\n",
      "loss 1.086089\n",
      "loss 1.109508\n",
      "loss 1.002555\n",
      "loss 0.735774\n",
      "loss 0.965851\n",
      "loss 0.895245\n",
      "loss 1.501767\n",
      "loss 1.086454\n",
      "loss 0.917009\n",
      "loss 1.406702\n",
      "loss 0.802838\n",
      "loss 0.935504\n",
      "loss 1.411372\n",
      "loss 0.876022\n",
      "loss 0.742671\n",
      "loss 0.572070\n",
      "loss 0.998541\n",
      "loss 1.265150\n",
      "loss 1.399615\n",
      "loss 0.808444\n",
      "loss 0.811231\n",
      "loss 0.533483\n",
      "loss 1.246485\n",
      "loss 0.801252\n",
      "loss 0.812562\n",
      "loss 0.977859\n",
      "loss 0.960054\n",
      "loss 1.084405\n",
      "loss 0.822301\n",
      "loss 0.740205\n",
      "loss 1.344452\n",
      "loss 2.162998\n",
      "loss 0.981414\n",
      "loss 1.095686\n",
      "loss 0.781060\n",
      "loss 0.986007\n",
      "loss 0.796921\n",
      "loss 1.165670\n",
      "loss 0.963210\n",
      "loss 1.304036\n",
      "loss 1.000884\n",
      "loss 0.781867\n",
      "loss 0.987537\n",
      "loss 0.754576\n",
      "loss 1.067320\n",
      "loss 0.736886\n",
      "loss 1.119476\n",
      "loss 1.300895\n",
      "loss 0.627084\n",
      "loss 0.754929\n",
      "loss 1.395430\n",
      "loss 1.264347\n",
      "loss 0.749574\n",
      "loss 1.285680\n",
      "loss 1.221045\n",
      "loss 0.683518\n",
      "loss 0.633066\n",
      "loss 1.216136\n",
      "loss 0.717488\n",
      "loss 1.325862\n",
      "loss 0.903618\n",
      "loss 1.371199\n",
      "loss 1.000332\n",
      "loss 0.525357\n",
      "loss 0.644576\n",
      "loss 1.085745\n",
      "loss 1.189710\n",
      "loss 0.640598\n",
      "loss 0.712898\n",
      "loss 1.120997\n",
      "loss 0.696318\n",
      "loss 1.510268\n",
      "loss 0.960107\n",
      "loss 1.055624\n",
      "loss 0.658126\n",
      "loss 0.765100\n",
      "loss 0.800385\n",
      "loss 0.952792\n",
      "loss 1.019716\n",
      "loss 1.265466\n",
      "loss 0.858838\n",
      "loss 1.155185\n",
      "loss 0.801383\n",
      "loss 1.076437\n",
      "loss 0.793515\n",
      "loss 0.586769\n",
      "loss 0.847794\n",
      "loss 0.788188\n",
      "loss 0.831740\n",
      "loss 0.621255\n",
      "loss 1.020584\n",
      "loss 1.570584\n",
      "loss 0.507577\n",
      "loss 0.499936\n",
      "loss 0.826019\n",
      "loss 0.729437\n",
      "loss 0.377186\n",
      "loss 0.866797\n",
      "loss 1.358597\n",
      "loss 0.843582\n",
      "loss 0.666852\n",
      "loss 0.707903\n",
      "loss 1.702839\n",
      "loss 0.979826\n",
      "loss 0.901132\n",
      "loss 1.599491\n",
      "loss 1.283301\n",
      "loss 0.626342\n",
      "loss 0.871925\n",
      "loss 0.900195\n",
      "loss 1.307237\n",
      "loss 0.959746\n",
      "loss 0.688199\n",
      "loss 0.894477\n",
      "loss 1.032243\n",
      "loss 0.981829\n",
      "loss 1.070319\n",
      "loss 0.634669\n",
      "loss 0.603393\n",
      "loss 1.546414\n",
      "loss 1.170869\n",
      "loss 1.049314\n",
      "loss 0.806882\n",
      "loss 0.776385\n",
      "loss 1.392045\n",
      "loss 1.003026\n",
      "loss 0.853597\n",
      "loss 1.228951\n",
      "loss 0.998754\n",
      "loss 1.371034\n",
      "loss 1.206614\n",
      "loss 1.056699\n",
      "loss 0.751904\n",
      "loss 0.610960\n",
      "loss 1.284845\n",
      "loss 0.636310\n",
      "loss 0.853522\n",
      "loss 0.644660\n",
      "loss 0.692887\n",
      "loss 0.786384\n",
      "loss 0.558731\n",
      "loss 1.154329\n",
      "loss 1.002455\n",
      "loss 1.033944\n",
      "loss 0.439614\n",
      "loss 0.928122\n",
      "loss 0.673642\n",
      "loss 0.609349\n",
      "loss 1.202658\n",
      "loss 1.412785\n",
      "loss 1.204496\n",
      "loss 0.715745\n",
      "loss 0.790976\n",
      "loss 0.703275\n",
      "loss 1.255513\n",
      "loss 0.753235\n",
      "loss 1.270053\n",
      "loss 1.318369\n",
      "loss 1.335831\n",
      "loss 1.509262\n",
      "loss 0.715771\n",
      "loss 1.145741\n",
      "loss 0.832675\n",
      "loss 0.588594\n",
      "loss 0.903378\n",
      "loss 1.081838\n",
      "loss 0.894606\n",
      "loss 1.832756\n",
      "loss 1.732010\n",
      "loss 1.189623\n",
      "loss 0.694386\n",
      "loss 0.748555\n",
      "loss 1.407579\n",
      "loss 0.779264\n",
      "loss 0.859711\n",
      "loss 0.488236\n",
      "loss 1.149136\n",
      "loss 0.702418\n",
      "loss 0.890830\n",
      "loss 0.853852\n",
      "loss 1.064897\n",
      "loss 0.711750\n",
      "loss 0.856971\n",
      "loss 1.658844\n",
      "loss 0.771109\n",
      "loss 0.905073\n",
      "loss 0.827013\n",
      "loss 1.083903\n",
      "loss 1.472235\n",
      "loss 1.612228\n",
      "loss 0.898605\n",
      "loss 0.768280\n",
      "loss 1.046502\n",
      "loss 1.038387\n",
      "loss 0.709222\n",
      "loss 0.571499\n",
      "loss 0.787577\n",
      "loss 0.605942\n",
      "loss 1.000518\n",
      "loss 1.539621\n",
      "loss 0.514565\n",
      "loss 1.246335\n",
      "loss 0.329873\n",
      "loss 1.096980\n",
      "loss 1.311120\n",
      "loss 0.884842\n",
      "loss 0.985865\n",
      "loss 1.287052\n",
      "loss 1.310135\n",
      "loss 0.891811\n",
      "loss 0.864390\n",
      "loss 1.076267\n",
      "loss 0.673509\n",
      "loss 1.423335\n",
      "loss 0.802176\n",
      "loss 1.071974\n",
      "loss 1.053776\n",
      "loss 1.279932\n",
      "loss 1.083406\n",
      "loss 1.831523\n",
      "loss 0.760026\n",
      "loss 0.953231\n",
      "loss 0.616126\n",
      "loss 1.302265\n",
      "loss 0.871968\n",
      "loss 1.368669\n",
      "loss 1.051604\n",
      "loss 1.191741\n",
      "loss 0.814381\n",
      "loss 0.725202\n",
      "loss 0.809781\n",
      "loss 0.738168\n",
      "loss 0.661866\n",
      "loss 0.704398\n",
      "loss 1.287993\n",
      "loss 1.033567\n",
      "loss 0.664576\n",
      "loss 1.342486\n",
      "loss 1.413630\n",
      "loss 0.716705\n",
      "loss 0.725952\n",
      "loss 1.484325\n",
      "loss 0.850552\n",
      "loss 0.502087\n",
      "loss 1.006644\n",
      "loss 0.523919\n",
      "loss 0.992832\n",
      "loss 1.483630\n",
      "loss 1.313494\n",
      "loss 0.698845\n",
      "loss 0.767532\n",
      "loss 0.720652\n",
      "loss 1.300842\n",
      "loss 1.048309\n",
      "loss 1.175118\n",
      "loss 1.157580\n",
      "loss 0.900507\n",
      "loss 1.034577\n",
      "loss 0.949297\n",
      "loss 0.683206\n",
      "loss 0.766360\n",
      "loss 1.157387\n",
      "loss 0.626437\n",
      "loss 1.195730\n",
      "loss 2.116678\n",
      "loss 0.726249\n",
      "loss 0.934535\n",
      "loss 0.842627\n",
      "loss 0.659477\n",
      "loss 1.153172\n",
      "loss 0.793040\n",
      "loss 0.944109\n",
      "loss 1.148479\n",
      "loss 0.821570\n",
      "loss 0.663274\n",
      "loss 1.121725\n",
      "loss 0.895418\n",
      "loss 1.322533\n",
      "loss 2.302234\n",
      "loss 1.939396\n",
      "loss 0.724917\n",
      "loss 0.744737\n",
      "loss 1.280519\n",
      "loss 0.413709\n",
      "loss 0.821287\n",
      "loss 0.895946\n",
      "loss 1.046782\n",
      "loss 0.524870\n",
      "loss 1.065299\n",
      "loss 0.932598\n",
      "loss 0.964604\n",
      "loss 1.469448\n",
      "loss 0.969814\n",
      "loss 0.742171\n",
      "loss 1.136630\n",
      "loss 0.675247\n",
      "loss 0.978378\n",
      "loss 0.735755\n",
      "loss 0.539329\n",
      "loss 0.658221\n",
      "loss 1.392508\n",
      "loss 0.662407\n",
      "loss 0.733339\n",
      "loss 0.892907\n",
      "loss 0.791056\n",
      "loss 0.488041\n",
      "loss 0.830980\n",
      "loss 1.334162\n",
      "loss 1.050936\n",
      "loss 1.178892\n",
      "loss 1.120052\n",
      "loss 0.937658\n",
      "loss 0.634007\n",
      "loss 0.617726\n",
      "loss 1.696384\n",
      "loss 0.822296\n",
      "loss 0.867092\n",
      "loss 1.145358\n",
      "loss 0.760467\n",
      "loss 0.813664\n",
      "loss 1.188520\n",
      "loss 0.984909\n",
      "loss 1.053058\n",
      "loss 0.424303\n",
      "loss 0.428248\n",
      "loss 1.117858\n",
      "loss 0.932030\n",
      "loss 0.498615\n",
      "loss 0.875749\n",
      "tf.Tensor(\n",
      "[[6.48585200e-01 1.49828782e-02 1.02934524e-01 2.21999094e-01\n",
      "  1.14983330e-02]\n",
      " [1.53063595e-01 1.22915618e-02 4.03487943e-02 7.92079747e-01\n",
      "  2.21631839e-03]\n",
      " [3.25602502e-01 2.51639578e-02 3.01185548e-01 2.89551437e-01\n",
      "  5.84965311e-02]\n",
      " [4.82424408e-01 3.24057728e-01 4.95561101e-02 3.94793898e-02\n",
      "  1.04482457e-01]\n",
      " [4.24752943e-02 8.05038288e-02 2.88309008e-01 3.18135391e-03\n",
      "  5.85530519e-01]\n",
      " [2.30505943e-01 5.19919157e-01 1.00413464e-01 8.69097039e-02\n",
      "  6.22517280e-02]\n",
      " [3.78271751e-02 1.06537971e-03 7.54802749e-02 8.84925604e-01\n",
      "  7.01553479e-04]\n",
      " [2.10742816e-01 6.76020622e-01 2.21334845e-02 5.77221438e-03\n",
      "  8.53309408e-02]\n",
      " [1.05668628e-03 3.71708331e-04 9.32224989e-01 3.15174431e-04\n",
      "  6.60314634e-02]\n",
      " [7.23344207e-01 4.51388434e-02 2.72487123e-02 7.99158495e-03\n",
      "  1.96276665e-01]], shape=(10, 5), dtype=float32)\n",
      "loss 1.200571\n",
      "loss 0.868330\n",
      "loss 0.848174\n",
      "loss 1.310674\n",
      "loss 0.558715\n",
      "loss 1.690753\n",
      "loss 1.155901\n",
      "loss 1.273613\n",
      "loss 0.708470\n",
      "loss 1.856525\n",
      "loss 0.612587\n",
      "loss 1.159996\n",
      "loss 1.433194\n",
      "loss 0.459195\n",
      "loss 0.691033\n",
      "loss 0.673000\n",
      "loss 0.761961\n",
      "loss 1.396884\n",
      "loss 0.798892\n",
      "loss 0.521368\n",
      "loss 0.669421\n",
      "loss 1.002982\n",
      "loss 0.726559\n",
      "loss 1.736192\n",
      "loss 0.595536\n",
      "loss 0.896826\n",
      "loss 0.873439\n",
      "loss 0.347914\n",
      "loss 0.475586\n",
      "loss 0.571297\n",
      "loss 0.853033\n",
      "loss 1.204923\n",
      "loss 1.175665\n",
      "loss 0.922433\n",
      "loss 1.196391\n",
      "loss 1.138368\n",
      "loss 1.735827\n",
      "loss 1.303092\n",
      "loss 0.742685\n",
      "loss 1.701213\n",
      "loss 0.511690\n",
      "loss 0.952156\n",
      "loss 0.908090\n",
      "loss 0.940882\n",
      "loss 1.043333\n",
      "loss 1.091019\n",
      "loss 0.772484\n",
      "loss 1.071481\n",
      "loss 0.693069\n",
      "loss 0.501217\n",
      "loss 1.168208\n",
      "loss 0.658849\n",
      "loss 0.627641\n",
      "loss 1.169817\n",
      "loss 0.837317\n",
      "loss 0.744037\n",
      "loss 1.072860\n",
      "loss 0.747589\n",
      "loss 1.075585\n",
      "loss 1.323567\n",
      "loss 1.365498\n",
      "loss 0.863886\n",
      "loss 0.843361\n",
      "loss 0.901011\n",
      "loss 0.685367\n",
      "loss 0.656149\n",
      "loss 0.548163\n",
      "loss 0.494068\n",
      "loss 0.837018\n",
      "loss 0.367214\n",
      "loss 0.646942\n",
      "loss 1.189709\n",
      "loss 0.877332\n",
      "loss 0.757634\n",
      "loss 0.767595\n",
      "loss 1.581633\n",
      "loss 1.385515\n",
      "loss 1.258791\n",
      "loss 1.135202\n",
      "loss 0.992116\n",
      "loss 1.228478\n",
      "loss 1.464132\n",
      "loss 0.808123\n",
      "loss 0.691053\n",
      "loss 0.986186\n",
      "loss 0.875529\n",
      "loss 1.235400\n",
      "loss 0.564440\n",
      "loss 0.557198\n",
      "loss 0.865175\n",
      "loss 0.978573\n",
      "loss 0.884035\n",
      "loss 0.703276\n",
      "loss 0.843379\n",
      "loss 1.428469\n",
      "loss 1.090094\n",
      "loss 1.489090\n",
      "loss 1.038086\n",
      "loss 0.506576\n",
      "loss 1.321087\n",
      "loss 1.246851\n",
      "loss 0.770514\n",
      "loss 1.162185\n",
      "loss 1.016057\n",
      "loss 1.935265\n",
      "loss 1.125632\n",
      "loss 1.000408\n",
      "loss 1.376311\n",
      "loss 1.498171\n",
      "loss 0.831069\n",
      "loss 0.900132\n",
      "loss 0.509059\n",
      "loss 0.834486\n",
      "loss 0.964880\n",
      "loss 1.125549\n",
      "loss 0.537077\n",
      "loss 1.426961\n",
      "loss 1.030337\n",
      "loss 0.498753\n",
      "loss 0.890196\n",
      "loss 0.582910\n",
      "loss 1.516800\n",
      "loss 0.821338\n",
      "loss 1.426556\n",
      "loss 0.878793\n",
      "loss 0.913202\n",
      "loss 0.870103\n",
      "loss 1.185480\n",
      "loss 1.445936\n",
      "loss 0.901560\n",
      "loss 0.861021\n",
      "loss 1.407492\n",
      "loss 0.932189\n",
      "loss 1.361329\n",
      "loss 0.752787\n",
      "loss 0.854459\n",
      "loss 1.113524\n",
      "loss 1.584160\n",
      "loss 1.128137\n",
      "loss 0.538008\n",
      "loss 1.402753\n",
      "loss 1.234782\n",
      "loss 0.973280\n",
      "loss 0.948810\n",
      "loss 0.862910\n",
      "loss 0.891704\n",
      "loss 0.867008\n",
      "loss 1.452822\n",
      "loss 0.685892\n",
      "loss 0.743357\n",
      "loss 1.185513\n",
      "loss 1.012248\n",
      "loss 0.771980\n",
      "loss 0.913284\n",
      "loss 0.951469\n",
      "loss 0.861926\n",
      "loss 0.949632\n",
      "loss 0.905325\n",
      "loss 1.150308\n",
      "loss 1.116038\n",
      "loss 0.601210\n",
      "loss 0.490518\n",
      "loss 0.926568\n",
      "loss 1.040659\n",
      "loss 0.613393\n",
      "loss 0.967640\n",
      "loss 0.747090\n",
      "loss 0.920664\n",
      "loss 1.280347\n",
      "loss 1.236585\n",
      "loss 1.271845\n",
      "loss 1.175838\n",
      "loss 1.087848\n",
      "loss 0.666217\n",
      "loss 0.814393\n",
      "loss 1.158508\n",
      "loss 0.637983\n",
      "loss 1.490671\n",
      "loss 0.633856\n",
      "loss 1.981784\n",
      "loss 0.858046\n",
      "loss 0.904848\n",
      "loss 0.745901\n",
      "loss 1.272506\n",
      "loss 0.998261\n",
      "loss 0.761606\n",
      "loss 0.979863\n",
      "loss 0.771137\n",
      "loss 1.107884\n",
      "loss 0.537393\n",
      "loss 0.607853\n",
      "loss 1.772359\n",
      "loss 1.019420\n",
      "loss 0.953459\n",
      "loss 0.896511\n",
      "loss 0.495218\n",
      "loss 1.172500\n",
      "loss 1.306653\n",
      "loss 1.445964\n",
      "loss 0.692313\n",
      "loss 0.711295\n",
      "loss 0.863013\n",
      "loss 0.509508\n",
      "loss 0.910108\n",
      "loss 1.062684\n",
      "loss 0.880449\n",
      "loss 0.658662\n",
      "loss 1.317052\n",
      "loss 0.931759\n",
      "loss 0.915463\n",
      "loss 0.984291\n",
      "loss 0.667814\n",
      "loss 0.367571\n",
      "loss 1.204373\n",
      "loss 0.522512\n",
      "loss 0.963129\n",
      "loss 1.113396\n",
      "loss 1.359711\n",
      "loss 0.463444\n",
      "loss 1.232807\n",
      "loss 2.203187\n",
      "loss 1.401367\n",
      "loss 1.345010\n",
      "loss 0.970269\n",
      "loss 0.924690\n",
      "loss 0.983560\n",
      "loss 0.584263\n",
      "loss 0.728844\n",
      "loss 0.831597\n",
      "loss 0.900346\n",
      "loss 0.613693\n",
      "loss 0.975674\n",
      "loss 1.422348\n",
      "loss 0.790760\n",
      "loss 1.218794\n",
      "loss 0.609045\n",
      "loss 0.967267\n",
      "loss 0.897764\n",
      "loss 1.126221\n",
      "loss 0.855235\n",
      "loss 1.316600\n",
      "loss 1.353427\n",
      "loss 1.079427\n",
      "loss 1.129278\n",
      "loss 0.638022\n",
      "loss 0.922157\n",
      "loss 1.007949\n",
      "loss 0.904051\n",
      "loss 0.899184\n",
      "loss 0.809432\n",
      "loss 1.034749\n",
      "loss 0.869402\n",
      "loss 0.915146\n",
      "loss 0.942995\n",
      "loss 0.521596\n",
      "loss 0.981136\n",
      "loss 0.828450\n",
      "loss 0.492837\n",
      "loss 0.872816\n",
      "loss 0.399651\n",
      "loss 1.048837\n",
      "loss 0.727670\n",
      "loss 1.513183\n",
      "loss 0.923777\n",
      "loss 0.418168\n",
      "loss 1.060893\n",
      "loss 0.697791\n",
      "loss 1.594790\n",
      "loss 1.246354\n",
      "loss 1.500390\n",
      "loss 1.143498\n",
      "loss 0.860964\n",
      "loss 1.042056\n",
      "loss 0.441394\n",
      "loss 0.594205\n",
      "loss 1.061486\n",
      "loss 0.750791\n",
      "loss 0.603299\n",
      "loss 1.188008\n",
      "loss 0.668058\n",
      "loss 0.694398\n",
      "loss 0.914703\n",
      "loss 0.976600\n",
      "loss 1.508462\n",
      "loss 1.170164\n",
      "loss 1.073729\n",
      "loss 0.904870\n",
      "loss 1.496612\n",
      "loss 0.523241\n",
      "loss 0.846902\n",
      "loss 0.612096\n",
      "loss 0.456776\n",
      "loss 1.054668\n",
      "loss 0.593236\n",
      "loss 1.211564\n",
      "loss 0.640153\n",
      "loss 1.106756\n",
      "loss 0.762851\n",
      "loss 1.354206\n",
      "loss 1.180656\n",
      "loss 0.448231\n",
      "loss 1.145896\n",
      "loss 0.472016\n",
      "loss 1.157004\n",
      "loss 0.825320\n",
      "loss 0.777480\n",
      "loss 0.868121\n",
      "loss 0.967225\n",
      "loss 1.231362\n",
      "loss 0.575309\n",
      "loss 0.583692\n",
      "loss 1.120769\n",
      "loss 0.605344\n",
      "loss 1.515352\n",
      "loss 0.632843\n",
      "loss 0.704385\n",
      "loss 1.041985\n",
      "loss 0.584464\n",
      "loss 0.656358\n",
      "loss 1.032236\n",
      "loss 1.123527\n",
      "loss 0.624328\n",
      "loss 0.761903\n",
      "loss 1.014048\n",
      "loss 1.480954\n",
      "loss 1.305663\n",
      "loss 1.239006\n",
      "loss 0.952048\n",
      "loss 0.808680\n",
      "loss 1.026369\n",
      "loss 1.431760\n",
      "loss 0.605303\n",
      "loss 0.808717\n",
      "loss 1.362901\n",
      "loss 0.728994\n",
      "loss 0.766069\n",
      "loss 0.667551\n",
      "loss 1.150490\n",
      "loss 0.941952\n",
      "loss 0.589136\n",
      "loss 1.496893\n",
      "loss 0.735850\n",
      "loss 0.710955\n",
      "loss 0.581696\n",
      "loss 0.531859\n",
      "loss 1.714475\n",
      "loss 0.850009\n",
      "loss 1.492365\n",
      "loss 0.528573\n",
      "loss 0.548236\n",
      "loss 0.375389\n",
      "loss 1.039112\n",
      "loss 0.649287\n",
      "loss 0.546440\n",
      "loss 0.711152\n",
      "loss 0.693500\n",
      "loss 1.140620\n",
      "loss 0.846514\n",
      "loss 0.414671\n",
      "loss 0.947717\n",
      "loss 0.956147\n",
      "loss 0.716469\n",
      "loss 1.323492\n",
      "loss 1.270918\n",
      "loss 0.623059\n",
      "loss 1.295643\n",
      "loss 0.786232\n",
      "tf.Tensor(\n",
      "[[0.37657872 0.14302789 0.17979707 0.04540115 0.25519517]\n",
      " [0.05340554 0.82812357 0.02903753 0.00871958 0.08071374]\n",
      " [0.02546089 0.00209648 0.32104462 0.6448432  0.00655479]\n",
      " [0.4165     0.00948227 0.0625615  0.47445637 0.0369998 ]\n",
      " [0.00349039 0.00130155 0.7565888  0.00949146 0.2291278 ]\n",
      " [0.20677686 0.04974595 0.13924016 0.52884036 0.07539668]\n",
      " [0.08069325 0.00317938 0.12005912 0.7884409  0.00762735]\n",
      " [0.53402394 0.06590051 0.08378589 0.02633345 0.2899562 ]\n",
      " [0.00818518 0.9135903  0.01752493 0.00292857 0.05777099]\n",
      " [0.09884743 0.00981526 0.46371213 0.02092482 0.40670043]], shape=(10, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "num_epoch = 5\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = tfds.load(\"tf_flowers\", split=tfds.Split.TRAIN, as_supervised=True)\n",
    "dataset = dataset.map(lambda img, label: (tf.image.resize(img, (224, 224)) / 255.0, label)).shuffle(1024).batch(batch_size)\n",
    "model = tf.keras.applications.MobileNetV2(weights=None, classes=5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for e in range(num_epoch):\n",
    "    for images, labels in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(images, training=True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print(\"loss %f\" % loss.numpy())\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "    print(labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 224, 224, 3])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Split('train')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.Split.TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 7)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow 的图像表示为 [图像数目，长，宽，色彩通道数] 的四维张量\n",
    "# 这里我们的输入图像 image 的张量形状为 [1, 7, 7, 1]\n",
    "image = np.array([[\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 2, 1, 0],\n",
    "    [0, 0, 2, 2, 0, 1, 0],\n",
    "    [0, 1, 1, 0, 2, 1, 0],\n",
    "    [0, 0, 2, 1, 1, 0, 0],\n",
    "    [0, 2, 1, 1, 2, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0]\n",
    "]], dtype=np.float32)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.expand_dims(image, axis=-1)  \n",
    "W = np.array([[\n",
    "    [ 0, 0, -1], \n",
    "    [ 0, 1, 0 ], \n",
    "    [-2, 0, 2 ]\n",
    "]], dtype=np.float32)\n",
    "b = np.array([1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 7, 1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证码数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证码数据，先用之前切好到字符的，之前是向量，27*27拉平了，这里先转成矩阵\n",
    "with open('../data/train_x.txt') as xfile:\n",
    "    m_x = [[float(num) for num in line.split(',')] for line in xfile]\n",
    "\n",
    "with open('../data/train_y.txt') as yfile:\n",
    "    label_y = [ line.split(',')[0] for line in yfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_vc = np.matrix(m_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5005, 729)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_vc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_vc_reshape = np.array(train_x_vc).reshape(5005, 27, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_vc_reshape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把label转成数字\n",
    "LABEL_TEXT = list(string.digits) + list(string.ascii_letters)\n",
    "# LABEL_TEXT\n",
    "train_y_vc = [LABEL_TEXT.index(x) for x in label_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8:8\n",
      "Z:61\n",
      "2:2\n",
      "3:3\n",
      "3:3\n",
      "9:9\n",
      "J:45\n",
      "R:53\n",
      "Q:52\n",
      "H:43\n"
     ]
    }
   ],
   "source": [
    "for label_str, label_num in zip(label_y[:10], train_y_vc[:10]):\n",
    "    print(label_str + ':' + str(label_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "X = tf.convert_to_tensor(train_x_vc_reshape)\n",
    "Y = tf.convert_to_tensor(train_y_vc)\n",
    "vc_dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "\n",
    "train_size = 4000\n",
    "vc_train_dataset = vc_dataset.take(train_size)\n",
    "vc_test_dataset = vc_dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vc_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "tf.Tensor(\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]], shape=(10, 27, 27), dtype=float64)\n",
      "label\n",
      "tf.Tensor([ 8 61  2  3  3  9 45 53 52 43], shape=(10,), dtype=int32)\n",
      "data\n",
      "tf.Tensor(\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]], shape=(10, 27, 27), dtype=float64)\n",
      "label\n",
      "tf.Tensor([15  6  6  7  4 41 30 40  3  2], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x1 = vc_train_dataset.batch(10)\n",
    "\n",
    "stopsign = 0\n",
    "for x_data, x_label in x1:\n",
    "    print(\"data\")\n",
    "    print(x_data)\n",
    "    print(\"label\")\n",
    "    print(x_label)\n",
    "    stopsign += 1\n",
    "    if stopsign > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer ml_p1_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer ml_p1_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 4.220487\n",
      "batch 0: loss 4.130552\n",
      "batch 0: loss 3.883441\n",
      "batch 0: loss 4.210034\n",
      "batch 0: loss 3.996984\n",
      "batch 0: loss 3.813447\n",
      "batch 0: loss 3.816523\n",
      "batch 0: loss 3.357016\n",
      "batch 0: loss 3.896343\n",
      "batch 0: loss 3.785726\n",
      "batch 0: loss 3.491256\n",
      "batch 0: loss 3.110800\n",
      "batch 0: loss 3.633547\n",
      "batch 0: loss 3.551370\n",
      "batch 0: loss 3.351297\n",
      "batch 0: loss 3.430377\n",
      "batch 0: loss 3.114422\n",
      "batch 0: loss 3.675418\n",
      "batch 0: loss 3.436012\n",
      "batch 0: loss 3.428229\n",
      "batch 0: loss 2.605186\n",
      "batch 0: loss 3.596010\n",
      "batch 0: loss 3.465043\n",
      "batch 0: loss 2.943807\n",
      "batch 0: loss 3.026711\n",
      "batch 0: loss 2.720569\n",
      "batch 0: loss 2.790480\n",
      "batch 0: loss 2.231145\n",
      "batch 0: loss 2.708890\n",
      "batch 0: loss 2.360967\n",
      "batch 0: loss 3.247438\n",
      "batch 0: loss 2.453648\n",
      "batch 0: loss 2.398784\n",
      "batch 0: loss 1.820028\n",
      "batch 0: loss 2.091329\n",
      "batch 0: loss 2.554716\n",
      "batch 0: loss 3.564167\n",
      "batch 0: loss 2.403932\n",
      "batch 0: loss 1.420699\n",
      "batch 0: loss 2.529496\n",
      "batch 0: loss 3.157752\n",
      "batch 0: loss 1.711055\n",
      "batch 0: loss 2.389216\n",
      "batch 0: loss 2.102521\n",
      "batch 0: loss 2.757900\n",
      "batch 0: loss 1.836665\n",
      "batch 0: loss 1.628135\n",
      "batch 0: loss 1.629348\n",
      "batch 0: loss 1.940437\n",
      "batch 0: loss 2.288659\n",
      "batch 0: loss 1.943325\n",
      "batch 0: loss 1.994501\n",
      "batch 0: loss 1.874554\n",
      "batch 0: loss 2.261013\n",
      "batch 0: loss 2.084825\n",
      "batch 0: loss 2.047017\n",
      "batch 0: loss 3.198122\n",
      "batch 0: loss 2.224246\n",
      "batch 0: loss 1.274644\n",
      "batch 0: loss 1.316241\n",
      "batch 0: loss 1.380259\n",
      "batch 0: loss 1.806930\n",
      "batch 0: loss 2.464516\n",
      "batch 0: loss 3.130062\n",
      "batch 0: loss 1.102170\n",
      "batch 0: loss 1.228491\n",
      "batch 0: loss 1.550801\n",
      "batch 0: loss 1.745124\n",
      "batch 0: loss 1.670744\n",
      "batch 0: loss 2.321756\n",
      "batch 0: loss 2.339921\n",
      "batch 0: loss 1.068600\n",
      "batch 0: loss 0.879128\n",
      "batch 0: loss 1.957369\n",
      "batch 0: loss 1.340704\n",
      "batch 0: loss 1.551610\n",
      "batch 0: loss 1.043140\n",
      "batch 0: loss 1.600005\n",
      "batch 0: loss 0.518960\n",
      "batch 0: loss 2.178079\n",
      "batch 0: loss 1.064886\n",
      "batch 0: loss 1.281509\n",
      "batch 0: loss 0.971142\n",
      "batch 0: loss 0.946954\n",
      "batch 0: loss 1.416313\n",
      "batch 0: loss 1.808470\n",
      "batch 0: loss 1.529703\n",
      "batch 0: loss 1.894257\n",
      "batch 0: loss 0.943340\n",
      "batch 0: loss 1.820679\n",
      "batch 0: loss 1.963273\n",
      "batch 0: loss 2.118120\n",
      "batch 0: loss 0.634070\n",
      "batch 0: loss 1.064285\n",
      "batch 0: loss 1.280900\n",
      "batch 0: loss 0.466231\n",
      "batch 0: loss 0.593114\n",
      "batch 0: loss 1.038358\n",
      "batch 0: loss 0.748258\n",
      "batch 0: loss 1.292200\n",
      "batch 0: loss 0.703869\n",
      "batch 0: loss 0.979271\n",
      "batch 0: loss 1.145183\n",
      "batch 0: loss 1.296995\n",
      "batch 0: loss 1.575700\n",
      "batch 0: loss 0.636210\n",
      "batch 0: loss 0.392803\n",
      "batch 0: loss 0.764919\n",
      "batch 0: loss 0.421590\n",
      "batch 0: loss 0.274462\n",
      "batch 0: loss 0.950468\n",
      "batch 0: loss 1.426027\n",
      "batch 0: loss 0.629979\n",
      "batch 0: loss 0.901366\n",
      "batch 0: loss 0.754633\n",
      "batch 0: loss 1.096849\n",
      "batch 0: loss 1.033970\n",
      "batch 0: loss 0.815187\n",
      "batch 0: loss 0.721218\n",
      "batch 0: loss 1.218952\n",
      "batch 0: loss 0.683479\n",
      "batch 0: loss 1.872192\n",
      "batch 0: loss 2.335195\n",
      "batch 0: loss 0.628277\n",
      "batch 0: loss 1.174182\n",
      "batch 0: loss 1.432201\n",
      "batch 0: loss 1.009756\n",
      "batch 0: loss 0.538247\n",
      "batch 0: loss 1.126130\n",
      "batch 0: loss 0.564569\n",
      "batch 0: loss 0.895550\n",
      "batch 0: loss 0.946869\n",
      "batch 0: loss 0.975652\n",
      "batch 0: loss 1.188332\n",
      "batch 0: loss 0.655231\n",
      "batch 0: loss 0.948914\n",
      "batch 0: loss 0.773509\n",
      "batch 0: loss 1.054474\n",
      "batch 0: loss 1.031657\n",
      "batch 0: loss 1.220736\n",
      "batch 0: loss 0.753150\n",
      "batch 0: loss 0.601109\n",
      "batch 0: loss 0.613208\n",
      "batch 0: loss 0.502332\n",
      "batch 0: loss 0.463763\n",
      "batch 0: loss 0.310331\n",
      "batch 0: loss 1.688289\n",
      "batch 0: loss 0.436153\n",
      "batch 0: loss 1.013503\n",
      "batch 0: loss 0.333099\n",
      "batch 0: loss 0.713372\n",
      "batch 0: loss 0.374755\n",
      "batch 0: loss 0.979786\n",
      "batch 0: loss 1.149979\n",
      "batch 0: loss 0.416658\n",
      "batch 0: loss 0.600537\n",
      "batch 0: loss 0.754691\n",
      "batch 0: loss 0.364935\n",
      "batch 0: loss 0.552673\n",
      "batch 0: loss 0.384951\n",
      "batch 0: loss 0.625600\n",
      "batch 0: loss 0.465133\n",
      "batch 0: loss 0.770975\n",
      "batch 0: loss 0.241153\n",
      "batch 0: loss 0.656677\n",
      "batch 0: loss 0.233339\n",
      "batch 0: loss 0.888454\n",
      "batch 0: loss 0.313335\n",
      "batch 0: loss 0.597497\n",
      "batch 0: loss 1.037170\n",
      "batch 0: loss 0.347279\n",
      "batch 0: loss 0.684204\n",
      "batch 0: loss 0.485265\n",
      "batch 0: loss 0.649744\n",
      "batch 0: loss 0.276341\n",
      "batch 0: loss 0.296165\n",
      "batch 0: loss 0.575276\n",
      "batch 0: loss 0.193581\n",
      "batch 0: loss 0.293959\n",
      "batch 0: loss 0.281218\n",
      "batch 0: loss 0.084816\n",
      "batch 0: loss 0.356315\n",
      "batch 0: loss 0.239140\n",
      "batch 0: loss 0.362445\n",
      "batch 0: loss 0.286746\n",
      "batch 0: loss 0.540365\n",
      "batch 0: loss 0.389800\n",
      "batch 0: loss 0.673375\n",
      "batch 0: loss 0.455873\n",
      "batch 0: loss 0.378355\n",
      "batch 0: loss 0.892185\n",
      "batch 0: loss 0.170835\n",
      "batch 0: loss 0.331662\n",
      "batch 0: loss 0.422567\n",
      "batch 0: loss 0.134930\n",
      "batch 0: loss 0.325459\n",
      "batch 0: loss 0.228580\n",
      "batch 0: loss 0.255906\n",
      "batch 0: loss 0.330801\n",
      "batch 0: loss 0.730078\n",
      "batch 0: loss 1.026532\n",
      "batch 0: loss 0.141408\n",
      "batch 0: loss 0.361633\n",
      "batch 0: loss 0.132536\n",
      "batch 0: loss 0.509442\n",
      "batch 0: loss 0.292515\n",
      "batch 0: loss 0.524316\n",
      "batch 0: loss 0.532297\n",
      "batch 0: loss 0.174254\n",
      "batch 0: loss 0.521370\n",
      "batch 0: loss 0.500718\n",
      "batch 0: loss 0.186098\n",
      "batch 0: loss 0.395205\n",
      "batch 0: loss 0.091665\n",
      "batch 0: loss 0.779738\n",
      "batch 0: loss 0.219141\n",
      "batch 0: loss 0.664756\n",
      "batch 0: loss 0.137738\n",
      "batch 0: loss 0.891437\n",
      "batch 0: loss 0.390918\n",
      "batch 0: loss 0.162619\n",
      "batch 0: loss 0.356953\n",
      "batch 0: loss 0.299804\n",
      "batch 0: loss 0.313436\n",
      "batch 0: loss 0.078119\n",
      "batch 0: loss 0.451483\n",
      "batch 0: loss 0.581896\n",
      "batch 0: loss 0.429554\n",
      "batch 0: loss 0.082666\n",
      "batch 0: loss 0.329812\n",
      "batch 0: loss 0.289857\n",
      "batch 0: loss 0.454075\n",
      "batch 0: loss 0.298743\n",
      "batch 0: loss 0.047741\n",
      "batch 0: loss 0.095708\n",
      "batch 0: loss 0.237404\n",
      "batch 0: loss 0.240425\n",
      "batch 0: loss 0.254412\n",
      "batch 0: loss 0.510041\n",
      "batch 0: loss 0.603575\n",
      "batch 0: loss 0.157872\n",
      "batch 0: loss 0.098494\n",
      "batch 0: loss 0.669329\n",
      "batch 0: loss 0.166298\n",
      "batch 0: loss 0.386764\n",
      "batch 0: loss 0.115815\n",
      "batch 0: loss 0.200093\n",
      "batch 0: loss 0.098335\n",
      "batch 0: loss 0.264087\n",
      "batch 0: loss 0.368928\n",
      "batch 0: loss 0.047234\n",
      "batch 0: loss 0.497553\n",
      "batch 0: loss 0.160015\n",
      "batch 0: loss 0.304579\n",
      "batch 0: loss 0.200526\n",
      "batch 0: loss 0.408488\n",
      "batch 0: loss 0.141163\n",
      "batch 0: loss 0.176011\n",
      "batch 0: loss 0.213276\n",
      "batch 0: loss 0.214906\n",
      "batch 0: loss 0.469430\n",
      "batch 0: loss 0.315685\n",
      "batch 0: loss 0.563493\n",
      "batch 0: loss 0.107293\n",
      "batch 0: loss 0.192316\n",
      "batch 0: loss 0.192464\n",
      "batch 0: loss 0.310133\n",
      "batch 0: loss 0.141718\n",
      "batch 0: loss 0.575146\n",
      "batch 0: loss 0.205737\n",
      "batch 0: loss 0.288518\n",
      "batch 0: loss 0.089609\n",
      "batch 0: loss 0.118000\n",
      "batch 0: loss 0.123345\n",
      "batch 0: loss 0.178564\n",
      "batch 0: loss 0.256894\n",
      "batch 0: loss 0.124562\n",
      "batch 0: loss 0.310056\n",
      "batch 0: loss 0.155325\n",
      "batch 0: loss 0.158622\n",
      "batch 0: loss 0.188955\n",
      "batch 0: loss 0.075139\n",
      "batch 0: loss 0.120672\n",
      "batch 0: loss 0.249047\n",
      "batch 0: loss 0.459657\n",
      "batch 0: loss 0.294678\n",
      "batch 0: loss 0.093438\n",
      "batch 0: loss 0.111902\n",
      "batch 0: loss 0.591405\n",
      "batch 0: loss 0.309133\n",
      "batch 0: loss 0.500823\n",
      "batch 0: loss 0.186067\n",
      "batch 0: loss 0.104666\n",
      "batch 0: loss 0.167547\n",
      "batch 0: loss 0.112900\n",
      "batch 0: loss 0.091223\n",
      "batch 0: loss 0.060898\n",
      "batch 0: loss 0.314692\n",
      "batch 0: loss 0.180190\n",
      "batch 0: loss 0.193998\n",
      "batch 0: loss 0.453219\n",
      "batch 0: loss 0.208732\n",
      "batch 0: loss 0.437685\n",
      "batch 0: loss 0.402456\n",
      "batch 0: loss 0.264658\n",
      "batch 0: loss 0.227737\n",
      "batch 0: loss 0.212937\n",
      "batch 0: loss 0.444724\n",
      "batch 0: loss 0.262350\n",
      "batch 0: loss 0.138416\n",
      "batch 0: loss 0.193302\n",
      "batch 0: loss 0.070465\n",
      "batch 0: loss 0.291268\n",
      "batch 0: loss 0.135440\n",
      "batch 0: loss 0.050423\n",
      "batch 0: loss 0.260368\n",
      "batch 0: loss 0.295830\n",
      "batch 0: loss 0.132713\n",
      "batch 0: loss 0.450948\n",
      "batch 0: loss 0.070558\n",
      "batch 0: loss 0.334962\n",
      "batch 0: loss 0.139971\n",
      "batch 0: loss 0.103053\n",
      "batch 0: loss 0.027933\n",
      "batch 0: loss 0.070348\n",
      "batch 0: loss 0.082748\n",
      "batch 0: loss 0.517814\n",
      "batch 0: loss 0.196467\n",
      "batch 0: loss 0.061694\n",
      "batch 0: loss 0.428826\n",
      "batch 0: loss 0.049796\n",
      "batch 0: loss 0.422624\n",
      "batch 0: loss 0.106224\n",
      "batch 0: loss 0.326433\n",
      "batch 0: loss 0.266070\n",
      "batch 0: loss 0.129988\n",
      "batch 0: loss 0.090247\n",
      "batch 0: loss 0.487497\n",
      "batch 0: loss 0.134454\n",
      "batch 0: loss 0.062787\n",
      "batch 0: loss 0.418473\n",
      "batch 0: loss 0.086026\n",
      "batch 0: loss 0.104290\n",
      "batch 0: loss 0.018993\n",
      "batch 0: loss 0.132625\n",
      "batch 0: loss 0.245984\n",
      "batch 0: loss 0.135379\n",
      "batch 0: loss 0.071161\n",
      "batch 0: loss 0.858929\n",
      "batch 0: loss 0.027744\n",
      "batch 0: loss 0.131951\n",
      "batch 0: loss 0.275762\n",
      "batch 0: loss 0.119152\n",
      "batch 0: loss 0.158626\n",
      "batch 0: loss 0.150310\n",
      "batch 0: loss 0.122596\n",
      "batch 0: loss 0.032545\n",
      "batch 0: loss 0.046080\n",
      "batch 0: loss 0.162175\n",
      "batch 0: loss 0.076803\n",
      "batch 0: loss 0.076670\n",
      "batch 0: loss 0.066408\n",
      "batch 0: loss 0.036799\n",
      "batch 0: loss 0.364757\n",
      "batch 0: loss 0.232782\n",
      "batch 0: loss 0.290390\n",
      "batch 0: loss 0.024842\n",
      "batch 0: loss 0.279022\n",
      "batch 0: loss 0.765468\n",
      "batch 0: loss 0.039961\n",
      "batch 0: loss 0.067548\n",
      "batch 0: loss 0.089394\n",
      "batch 0: loss 0.112019\n",
      "batch 0: loss 0.031134\n",
      "batch 0: loss 0.039568\n",
      "batch 0: loss 0.208050\n",
      "batch 0: loss 0.047453\n",
      "batch 0: loss 0.183836\n",
      "batch 0: loss 0.040478\n",
      "batch 0: loss 0.053974\n",
      "batch 0: loss 0.518103\n",
      "batch 0: loss 0.224193\n",
      "batch 0: loss 0.240072\n",
      "batch 0: loss 0.270274\n",
      "batch 0: loss 0.232474\n",
      "batch 0: loss 0.268808\n",
      "batch 0: loss 0.097050\n",
      "batch 0: loss 0.193282\n",
      "batch 0: loss 0.022447\n",
      "batch 0: loss 0.121994\n",
      "batch 0: loss 0.020408\n",
      "batch 0: loss 0.177688\n",
      "batch 0: loss 0.075420\n",
      "batch 0: loss 0.062870\n",
      "batch 0: loss 0.043599\n",
      "batch 0: loss 0.065736\n",
      "batch 0: loss 0.041003\n",
      "batch 0: loss 0.071350\n",
      "batch 0: loss 0.022455\n",
      "batch 0: loss 0.053260\n",
      "batch 0: loss 0.019890\n",
      "batch 0: loss 0.087686\n",
      "batch 0: loss 0.245003\n",
      "batch 0: loss 0.267879\n",
      "batch 0: loss 0.101083\n",
      "batch 0: loss 0.039007\n",
      "batch 0: loss 0.082176\n",
      "batch 0: loss 0.054873\n",
      "batch 0: loss 0.363726\n",
      "batch 0: loss 0.094614\n",
      "batch 0: loss 0.106095\n",
      "batch 0: loss 0.362867\n",
      "batch 0: loss 0.104363\n",
      "batch 0: loss 0.041195\n",
      "batch 0: loss 0.123889\n",
      "batch 0: loss 0.057938\n",
      "batch 0: loss 0.053450\n",
      "batch 0: loss 0.059784\n",
      "batch 0: loss 0.161802\n",
      "batch 0: loss 0.044311\n",
      "batch 0: loss 0.081052\n",
      "batch 0: loss 0.043452\n",
      "batch 0: loss 0.103294\n",
      "batch 0: loss 0.027219\n",
      "batch 0: loss 0.138468\n",
      "batch 0: loss 0.051564\n",
      "batch 0: loss 0.042497\n",
      "batch 0: loss 0.148589\n",
      "batch 0: loss 0.124408\n",
      "batch 0: loss 0.209772\n",
      "batch 0: loss 0.041979\n",
      "batch 0: loss 0.039913\n",
      "batch 0: loss 0.069496\n",
      "batch 0: loss 0.081264\n",
      "batch 0: loss 0.060319\n",
      "batch 0: loss 0.385022\n",
      "batch 0: loss 0.096798\n",
      "batch 0: loss 0.121514\n",
      "batch 0: loss 0.393361\n",
      "batch 0: loss 0.043724\n",
      "batch 0: loss 0.119369\n",
      "batch 0: loss 0.033077\n",
      "batch 0: loss 0.037754\n",
      "batch 0: loss 0.062045\n",
      "batch 0: loss 0.120781\n",
      "batch 0: loss 0.066486\n",
      "batch 0: loss 0.070416\n",
      "batch 0: loss 0.157801\n",
      "batch 0: loss 0.059918\n",
      "batch 0: loss 0.385443\n",
      "batch 0: loss 0.071153\n",
      "batch 0: loss 0.214913\n",
      "batch 0: loss 0.108354\n",
      "batch 0: loss 0.055384\n",
      "batch 0: loss 0.028799\n",
      "batch 0: loss 0.040553\n",
      "batch 0: loss 0.064945\n",
      "batch 0: loss 0.044564\n",
      "batch 0: loss 0.178564\n",
      "batch 0: loss 0.083313\n",
      "batch 0: loss 0.062950\n",
      "batch 0: loss 0.094894\n",
      "batch 0: loss 0.062902\n",
      "batch 0: loss 0.024873\n",
      "batch 0: loss 0.054742\n",
      "batch 0: loss 0.032120\n",
      "batch 0: loss 0.058356\n",
      "batch 0: loss 0.059585\n",
      "batch 0: loss 0.080430\n",
      "batch 0: loss 0.059333\n",
      "batch 0: loss 0.142533\n",
      "batch 0: loss 0.125491\n",
      "batch 0: loss 0.127091\n",
      "batch 0: loss 0.058516\n",
      "batch 0: loss 0.054512\n",
      "batch 0: loss 0.195712\n",
      "batch 0: loss 0.037884\n",
      "batch 0: loss 0.072074\n",
      "batch 0: loss 0.019496\n",
      "batch 0: loss 0.043373\n",
      "batch 0: loss 0.254906\n",
      "batch 0: loss 0.057833\n",
      "batch 0: loss 0.059259\n",
      "batch 0: loss 0.081054\n",
      "batch 0: loss 0.102248\n",
      "batch 0: loss 0.075030\n",
      "batch 0: loss 0.025921\n",
      "batch 0: loss 0.032087\n",
      "batch 0: loss 0.079610\n",
      "batch 0: loss 0.024644\n",
      "batch 0: loss 0.055716\n",
      "batch 0: loss 0.111727\n",
      "batch 0: loss 0.130925\n",
      "batch 0: loss 0.061813\n",
      "batch 0: loss 0.199226\n",
      "batch 0: loss 0.091813\n",
      "batch 0: loss 0.099025\n",
      "batch 0: loss 0.026944\n",
      "batch 0: loss 0.018293\n",
      "batch 0: loss 0.074466\n",
      "batch 0: loss 0.043115\n"
     ]
    }
   ],
   "source": [
    "class MLP1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=62)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "model_vc = MLP1()\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "num_batches = int( len(vc_train_dataset) // batch_size * num_epochs)\n",
    "vc_train_dataset_batches = vc_dataset.batch(batch_size)\n",
    "# for batch_index in range(num_batches):\n",
    "for X, y in vc_train_dataset_batches:\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model_vc(X)\n",
    "#         y_pred_single = y_pred.numpy().argmax(axis=1)\n",
    "#         y_pred_single = tf.math.argmax(y_pred, axis=1)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "#         loss = tf.keras.losses.categorical_crossentropy(y_true=y, y_pred=y_pred_single)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model_vc.variables)\n",
    "    _ = optimizer.apply_gradients(grads_and_vars=zip(grads, model_vc.variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.987250\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(len(vc_train_dataset) // batch_size)\n",
    "# for batch_index in range(num_batches):\n",
    "#     start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "vc_train_dataset_batches = vc_train_dataset.batch(batch_size)\n",
    "for X, y in vc_train_dataset_batches:\n",
    "    y_pred = model_vc.predict(X)\n",
    "#     y_pred_single = tf.math.argmax(y_pred, axis=1)\n",
    "    _ = sparse_categorical_accuracy.update_state(y_true=y, y_pred=y_pred)\n",
    "print(\"train accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.994030\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(len(vc_test_dataset) // batch_size)\n",
    "\n",
    "vc_test_dataset_batches = vc_test_dataset.batch(batch_size)\n",
    "for X, y in vc_test_dataset_batches:\n",
    "    y_pred = model_vc.predict(X)\n",
    "#     y_pred_single = tf.math.argmax(y_pred, axis=1)\n",
    "    _ = sparse_categorical_accuracy.update_state(y_true=y, y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
